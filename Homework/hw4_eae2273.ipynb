{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 371 Machine Learning for Text Analysis\n",
    "\n",
    "# Homework 4 - due Wednesday Apr 17 2024 at 11:59pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas:\n",
    "- a notebook renamed ``hw4_YourEID.ipynb``\n",
    "\n",
    "**Note**: This is a small homework and a perfect solution to this homework will be worth **100** points.\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs the correct answer.\n",
    "\n",
    "For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors.Â **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! There is a channel on Slack where you can look for a study group.\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jessyli.com/courses/lin371\n",
    "\n",
    "For typing up your answers to non-programming problems, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any collaborators here:\n",
    "- Rylan Vachon\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Multi-layer perceptron going linear (10 points)\n",
    "\n",
    "Show mathematically that a multi-layer perceptron using the _linear (identity) activation function_ $y=z$ is still a linear classifier. You can assume that (1) each example has two features $x_1$ and $x_2$; (2) there are two hidden layers; (3) each hidden layer has two nodes; (4) this is for a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Concepts (30 points)\n",
    "Briefly answer the following questions:\n",
    "\n",
    "**(1)** What is batching in neural networks, and why batching is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching in neural networks is when the training data is split up into several different \"batches\" of data rather than one large total set.\n",
    "\n",
    "It is used for several reasons:\n",
    "- Allows for more accuracy when estimating the gradient\n",
    "- Creates smoother convergence\n",
    "- Allows for larger learning rates\n",
    "- Improves speed and lessens load on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Name and describe two ways to prevent overfitting in a deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dropout is a method used to prevent overfitting in deep neural networks. It relies on randomly setting some activations in each layer to zero, generally about half of each layer. This method forces the network to rely less on any single node.\n",
    "2. Early stopping is another method used to prevent overfitting in deep neural networks. To do early stopping, you have to have an additional set of data called a \"development\" or \"validation\" set. This set helps to determine at what point in the training iterations the model will begin to overfit so we can then go back to the actual training set and know when to cease iterations to minimize the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** Name at least one benefit of using static word embeddings (e.g., glove, word2vec) over bag-of-words representations, and at least one aspect of language that word embeddings do not account for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One benefit of using static word embeddings compared to bag-of-words representations is that static word embeddings take semantic relationships into account. Static word embeddings vectorize words and show their similarity by their proximity in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** What's a key difference between static word embeddings (e.g., glove, word2vec) and contextualized word embeddings (e.g., BERT, Elmo)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contextualized word embeddings take into consideration the context of the text surrounding each word (river *bank* vs financial *bank*). Static word embeddings have a fixed (static) representation of each word, making it more difficult for the model to be flexible when encountering different contextual uses of the same word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** Suppose we are considering a tweet classification task on 8000 examples. A common way to obtain a prepresentation of one tweet via word embeddings is to take the average of the embeddings of each token within this tweet. Suppose we are designing a neural network with:\n",
    "* Word embeddings of 300 dimensions; \n",
    "* A first hidden layer with 100 hidden states;\n",
    "* A second hidden layer with 10 hidden states;\n",
    "* A final binary classification layer.\n",
    "\n",
    "How many parameters will the network learn? Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Bias in word embeddings (15 points)\n",
    "\n",
    "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
    "\n",
    "Here, we shall explore the embeddings produced by GloVe. Please revisit the class notes and lecture slides for more details on the word2vec and GloVe algorithms.\n",
    "\n",
    "Then run the following cells to load the GloVe vectors into memory. **Note**: If this is your first time to run these cells, i.e. download the embedding model, it will take a couple minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 400000\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    \n",
    "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
    "    print(\"Loaded vocab size %i\" % len(wv_from_bin.key_to_index))\n",
    "    return wv_from_bin\n",
    "\n",
    "# -----------------------------------\n",
    "# Run Cell to Load Word Vectors\n",
    "# Note: This will take a couple minutes\n",
    "# -----------------------------------\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"worker\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"worker\" and most dissimilar to \"woman\". **Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('employee', 0.6375863552093506), ('workers', 0.6068920493125916), ('nurse', 0.5837947130203247), ('pregnant', 0.5363885164260864), ('mother', 0.5321308970451355), ('employer', 0.5127025842666626), ('teacher', 0.5099576711654663), ('child', 0.5096741318702698), ('homemaker', 0.5019454956054688), ('nurses', 0.4970572590827942)]\n",
      "\n",
      "[('workers', 0.611325740814209), ('employee', 0.5983108878135681), ('working', 0.5615329742431641), ('laborer', 0.5442320108413696), ('unemployed', 0.536851704120636), ('job', 0.5278826355934143), ('work', 0.5223963856697083), ('mechanic', 0.5088937282562256), ('worked', 0.5054520964622498), ('factory', 0.4940454363822937)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
    "# most dissimilar from.\n",
    "print(wv_from_bin.most_similar(positive=['woman', 'worker'], negative=['man']))\n",
    "print()\n",
    "print(wv_from_bin.most_similar(positive=['man', 'worker'], negative=['woman']))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your anwser goes here\n",
    "# Note: before submitting the file, make sure the anwser cell is a markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Now, Use the most_similar function to find another case where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apartments', 0.5706000924110413), ('residential', 0.5501178503036499), ('administration', 0.5297499895095825), ('homes', 0.5222154259681702), ('house', 0.5085390210151672), ('hud', 0.49044468998908997), ('houses', 0.48522552847862244), ('building', 0.48337918519973755), ('buildings', 0.47960007190704346), ('multifamily', 0.47858700156211853)]\n",
      "\n",
      "[('residential', 0.5735326409339905), ('urban', 0.5647248029708862), ('apartments', 0.5408486127853394), ('sector', 0.5248250961303711), ('neighborhoods', 0.508280336856842), ('construction', 0.5015413165092468), ('rural', 0.49821096658706665), ('affordable', 0.49628469347953796), ('employment', 0.49382278323173523), ('communities', 0.4924072027206421)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: before submitting the file, make sure this anwser cell is a code cell\n",
    "# write code\n",
    "# put your explanation in the print statement:\n",
    "# e.g. print(\"Explanation: xxxx\")\n",
    "\n",
    "print(wv_from_bin.most_similar(positive=['white', 'housing'], negative=['black']))\n",
    "print()\n",
    "print(wv_from_bin.most_similar(positive=['black', 'housing'], negative=['white']))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** Give one explanation of how bias gets into the word vectors. What is an experiment that you could do to test for or to measure this source of bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: before submitting the file, make sure this anwser cell is a markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Multi-layer perceptron in sklearn (45 points)\n",
    "\n",
    "(a) (9 points) Using the Logistic Regression model you implemented in homework 3 as a starting point, modify this to use the [Multi-layer Perceptron model provided by sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) as the learning algorithm. How do the two models compare in terms of predictive performance (accuracy)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "dataset = load_files(\"movie-reviews/\")\n",
    "# generate train/test subsets\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size = 0.2, random_state = 3)\n",
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "X_train = vectorizer.fit_transform(docs_train)\n",
    "X_test = vectorizer.transform(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8475 0.8725\n"
     ]
    }
   ],
   "source": [
    "# Note: before submitting this file, make sure the anwser cell is a code cell\n",
    "# put your text explanation in the print statement:\n",
    "# e.g. print(\"xxx\")\n",
    "# You need to implement logistic regression model (lr_model) and multi-layer perceptron model (mlp_model) on the movie-reviews dataset.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#train the LR model\n",
    "lr_model = LogisticRegression(random_state=3, solver='liblinear')\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_y_scores = lr_model.predict(X_test)\n",
    "accuracy_score_lr = metrics.accuracy_score(y_test, lr_y_scores)\n",
    "\n",
    "#train the mlp model\n",
    "mlp_model = MLPClassifier(random_state=3)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_y_scores = mlp_model.predict(X_test)\n",
    "accuracy_score_mlp = metrics.accuracy_score(y_test, mlp_y_scores)\n",
    "\n",
    "print(accuracy_score_lr, accuracy_score_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(lr_model.solver == 'liblinear')\n",
    "assert(lr_model.random_state == 3)\n",
    "assert(mlp_model.random_state == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (9x4=36 points) Note that when you build the MLPClassifier, you will see a series of parameters printed out with the classifier. Consider the following parameters, modify them, and see how performance and/or training speed changes:\n",
    "\n",
    "**Note** that you should modify _only one_ parameter at a time! Additionally,  clearly label each time you test a parameter.\n",
    "\n",
    "(b.1) Turn early stopping on or off. **Also answer: what does early stopping do?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eaesp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Note: before submitting this file, make sure the anwser cell is a code cell\n",
    "# put your text explanation in the print statement:\n",
    "# e.g. print(\"xxx\")\n",
    "mlp_model = MLPClassifier(random_state=3, early_stopping=True)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_y_scores = mlp_model.predict(X_test)\n",
    "accuracy_score_mlp = metrics.accuracy_score(y_test, mlp_y_scores)\n",
    "print(accuracy_score_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b.2) Play with other activation functions. How does that affect performance? (identity, logistic, tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.865\n"
     ]
    }
   ],
   "source": [
    "# Note: before submitting this file, make sure the anwser cell is a code cell\n",
    "# put your text explanation in the print statement:\n",
    "# e.g. print(\"xxx\")\n",
    "mlp_model = MLPClassifier(random_state=3, activation='identity')\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_y_scores = mlp_model.predict(X_test)\n",
    "accuracy_score_mlp = metrics.accuracy_score(y_test, mlp_y_scores)\n",
    "print(accuracy_score_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8725\n"
     ]
    }
   ],
   "source": [
    "# Note: before submitting this file, make sure the anwser cell is a code cell\n",
    "# put your text explanation in the print statement:\n",
    "# e.g. print(\"xxx\")\n",
    "mlp_model = MLPClassifier(random_state=3, activation='logistic')\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_y_scores = mlp_model.predict(X_test)\n",
    "accuracy_score_mlp = metrics.accuracy_score(y_test, mlp_y_scores)\n",
    "print(accuracy_score_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87\n"
     ]
    }
   ],
   "source": [
    "# Note: before submitting this file, make sure the anwser cell is a code cell\n",
    "# put your text explanation in the print statement:\n",
    "# e.g. print(\"xxx\")\n",
    "mlp_model = MLPClassifier(random_state=3, activation='tanh')\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_y_scores = mlp_model.predict(X_test)\n",
    "accuracy_score_mlp = metrics.accuracy_score(y_test, mlp_y_scores)\n",
    "print(accuracy_score_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b.3) By default, MLPClassifier uses the *adam* algorithm to find weights, which is an adaptive method with changing learning rates. You can also choose stochastic gradient descent (sgd). Try it; how does sgd perform, compared with adam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eaesp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "# Note: before submitting this file, make sure the anwser cell is a code cell\n",
    "# put your text explanation in the print statement:\n",
    "# e.g. print(\"xxx\")\n",
    "mlp_model = MLPClassifier(random_state=3, solver='sgd')\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_y_scores = mlp_model.predict(X_test)\n",
    "accuracy_score_mlp = metrics.accuracy_score(y_test, mlp_y_scores)\n",
    "print(accuracy_score_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b.4) Experiment with different learning rates (parameter `learning_rate_init`), for example, try 0.1, 0.01, 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: before submitting this file, make sure the anwser cell is a code cell\n",
    "# put your text explanation in the print statement:\n",
    "# e.g. print(\"xxx\")\n",
    "mlp_model = MLPClassifier(random_state=3, learning_rate_init=0.1)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_y_scores = mlp_model.predict(X_test)\n",
    "accuracy_score_mlp = metrics.accuracy_score(y_test, mlp_y_scores)\n",
    "print(accuracy_score_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: before submitting this file, make sure the anwser cell is a code cell\n",
    "# put your text explanation in the print statement:\n",
    "# e.g. print(\"xxx\")\n",
    "mlp_model = MLPClassifier(random_state=3, learning_rate_init=0.01)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_y_scores = mlp_model.predict(X_test)\n",
    "accuracy_score_mlp = metrics.accuracy_score(y_test, mlp_y_scores)\n",
    "print(accuracy_score_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: before submitting this file, make sure the anwser cell is a code cell\n",
    "# put your text explanation in the print statement:\n",
    "# e.g. print(\"xxx\")\n",
    "mlp_model = MLPClassifier(random_state=3, learning_rate_init=0.001)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_y_scores = mlp_model.predict(X_test)\n",
    "accuracy_score_mlp = metrics.accuracy_score(y_test, mlp_y_scores)\n",
    "print(accuracy_score_mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
