{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 371 Machine Learning for Text Analysis\n",
    "\n",
    "# Homework 1 - due Monday Jan 29 2024 at 11:59pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas:\n",
    "- a notebook renamed ``hw1_YourEID.ipynb``\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs the correct answer. \n",
    "\n",
    "A perfect solution for this homework is worth 95 points but will be counted out of 100. If you completed homework 0, you will automatically receive an additional 5 points.  For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors. **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! There is a channel on Slack where you can look for a study group.\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jessyli.com/courses/lin371\n",
    "\n",
    "For typing up your answers to problems 1, 2 and 3, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any collaborators here:\n",
    "- Rylan Vachon\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: the paradox of induction (15 points)\n",
    "\n",
    "Consider a statement whose truth is unknown. If we see many examples that are compatible with it, we are tempted to view the statement as more probable. Such reasoning is often referred to as _inductive inference_ (in a philosophical, rather than mathematical sense). Consider now the statement that \"all cows are white\". An equivalent statement is that \"everything that is not white is not a cow\". We then observe several black panthers. Our observations are clearly compatible with the statement, but do they make the hypothesis \"all cows are white\" more likely?\n",
    "\n",
    "To analyze such a situation, we consider a probabilistic model. Let us assume that there are two possible states of the world, which we model as complementary events:\n",
    "\n",
    "<center> $A$: all cows are white,\n",
    "    \n",
    "<center> $A^c$: 50% of all cows are white.\n",
    "\n",
    "Let $p$ be the prior probability $P(A)$ that all cows are white. We make an observation of a cow or a panther, with probability $q$ and $1-q$, respectively, independent of whether event $A$ occurs or not. Assume that $0<p<1, 0<q<1$, and that all panthers are black.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Given the event $B=$\\{a black panther was observed\\}, what is $P(A|B)$? Show your work (5pts)\n",
    "\n",
    "If all panthers are black, then $P(B)$ = 1\n",
    "\n",
    "We need to find $P(A|B)$ which can be written as $\\dfrac {P(A \\cap B)}{P(B)}$\n",
    "\n",
    "* $P(A|B)$ = $\\dfrac {P(A \\cap B)}{P(B)}$\n",
    "\n",
    "    * $\\dfrac {P(A) \\cdot P(B)}{P(B)}$\n",
    "\n",
    "    * $P(B) = 1$ \n",
    "    \n",
    "    * $P(A) = p$\n",
    "    \n",
    "    * $\\dfrac {p \\cdot 1}{1}$\n",
    "\n",
    "    * $P(A|B) = p$\n",
    "\n",
    "Therefore, seeing a black panther does not improve the probability that all cows are white. The two events are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Given the event $C=$\\{a white cow was observed\\}, what was $P(A|C)$? Show your work (5pts)\n",
    "\n",
    "We need to find $P(A|C)$ using the Bayesian Model.\n",
    "\n",
    "* $P(A|C) = \\dfrac{P(C|A) \\cdot P(A)}{P(C)}$\n",
    "\n",
    "    * $\\dfrac{P(C|A) \\cdot p}{P(C)}$\n",
    "\n",
    "    * $\\dfrac{1 \\cdot p}{P(C)}$\n",
    "\n",
    "        * $\\dfrac{1 \\cdot 1}{P(C)}$\n",
    "\n",
    "        * $P(C)$ is the probability of randomly observing a white cow.\n",
    "\n",
    "        * $P(A|C) = \\dfrac{p}{P(C)}$\n",
    "\n",
    "            * $P(C) =  P(C,A) + P(C,A^C)$ We need to consider both the event $A$ and the complement $A^C$\n",
    "\n",
    "            * $(P(C|A) \\cdot P(A)) + (P(C|A^C) \\cdot P(A^C))$\n",
    "\n",
    "            * $(P(C|A) \\cdot p) + (P(C|A^C) \\cdot P(A^C))$\n",
    "\n",
    "            * $(1 \\cdot p) + (P(C|A^C) \\cdot P(A^C))$\n",
    "\n",
    "            * $(p) + (P(C|A^C) \\cdot P(A^C))$\n",
    "\n",
    "            * $(p) + (0.5 \\cdot P(A^C))$\n",
    "\n",
    "            * $p + (0.5 \\cdot (1-p))$\n",
    "\n",
    "            * $p + (0.5 - \\dfrac{1}{2p})$\n",
    "\n",
    "            * $\\dfrac{1}{2p} + 0.5$\n",
    "\n",
    "            * $P(C|A) = \\dfrac{1}{2p} + 0.5$\n",
    "        \n",
    "* $P(A|C) = \\dfrac{p}{\\dfrac{1}{2p} + 0.5}$\n",
    "\n",
    "The probability of $P(A|C)$ is $\\dfrac{p}{\\dfrac{1}{2p} + 0.5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Which is larger? Explain the implication. (5pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Problem 2: MAP (11 pts)\n",
    "\n",
    "We have discussed the Bernoulli distribution. Now, if you flip the coin $N$ times, with $H$ heads and $T$ tails, the likelihood of observing a sequence (data) $D$ is:\n",
    "$$\n",
    "P(D|\\theta) = \\theta^H(1-\\theta)^T\n",
    "$$\n",
    "\n",
    "In the Bayesian framework, we assume that we have some prior knowledge of $\\theta$ which can be described with a distribution $P(\\theta)$. \n",
    "The Beta distribution $Beta(\\alpha;\\beta)$ is often used as this prior, which, combined with our observed data $D$, leads to the posterior distribution $P(\\theta|D)$. \n",
    "\n",
    "But what if we want a single number (an estimate) of our \"best\" $\\theta$? This value will no longer be the same as the maximum likelihood estimate $\\hat{\\theta}_{MLE}$. Instead, this new \"best\" parameter, dubbed $\\hat{\\theta}_{MAP}$, is called _maximum a posterior_ or MAP: $$\\hat{\\theta}_{MAP}=\\text{argmax}_\\theta(P(\\theta|D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What is $\\hat{\\theta}_{MAP}$? Express it in terms of $H,T,\\alpha,\\beta$.\n",
    "\n",
    "$$\\hat{\\theta}_{MAP}=\\text{argmax}_\\theta(P(\\theta|D)$$\n",
    "\n",
    "${argmax}_\\theta(P(\\theta|D)$ =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Decision Trees (30 points)\n",
    "\n",
    "Consider the following set of training examples where we have two features, $X_1$ and $X_2$, and the goal is to predict the target $Y$. Each row indicates the values observed, and how many times that set of values was observed. For example, $(+,T,T)$ was observed 3 times, while $(−,T,T)$ was never observed.\n",
    "\n",
    "|Y | $$X_1$$ | $$X_2$$ | Count|\n",
    "|-|-|-|-|\n",
    "|+ | T | T | 3|\n",
    "|+ | T | F | 4|\n",
    "|+ | F | T | 4|\n",
    "|+ | F | F | 1|\n",
    "|- | T | T | 0|\n",
    "|- | T | F | 1|\n",
    "|- | F | T | 3|\n",
    "|- | F | F | 5 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) What is the sample entropy $H(Y)$ for this training data (with logarithms base 2), _before_ we start splitting on either feature? (10 pts)\n",
    "\n",
    "The equation for entropy is $H(Y) =$ $-\\sum{y} P(Y = y)log_{2}P(Y=y)$\n",
    "\n",
    "The base entropy for this training data would be:\n",
    "\n",
    "* $H(Y) = -P(Y=+)log_{2}P(Y=+) - P(Y=-)log{2}P(Y=-)$\n",
    "\n",
    "    * $(-\\dfrac{12}{21}\\cdot log_{2}\\dfrac{12}{21}) - (\\dfrac{9}{21}\\cdot log_{2}\\dfrac{9}{21})$\n",
    "\n",
    "    * $(-0.5714 \\cdot -0.8074) - (0.4285 \\cdot -1.2224)$\n",
    "\n",
    "    * $0.4613 - -0.5237$\n",
    "\n",
    "    * $H(Y) = 0.985$\n",
    "\n",
    "The sample entropy for $H(Y)$ before we start splitting on either feature is 0.985.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)  Should we first split on $X_1$ or $X_2$? Calculate the information gains for each feature. (20 pts)\n",
    "\n",
    "The forumla for conditional entropy is $H(Y|X) = P(X) + H(Y|X = x)$\n",
    "\n",
    "#### Information gain for $X_1$:\n",
    "##### Conditional Entropy for when $X_1$ = T\n",
    "\n",
    "* $H(Y|X=X_1=T) = (-\\dfrac{7}{8}\\cdot log_{2}\\dfrac{7}{8}) - (\\dfrac{1}{8}\\cdot log_{2}\\dfrac{1}{8})$\n",
    "\n",
    "    * $(-0.875 \\cdot -0.1926) - (0.125 \\cdot -3)$\n",
    "\n",
    "    * $0.1685 - -0.375$\n",
    "\n",
    "    * $H(Y|X=X_1=T) = 0.5435$\n",
    "\n",
    "    * $H(Y|X) = P(X=X_1=T) \\cdot H(Y|X=X_1=T)$\n",
    "\n",
    "        * $\\dfrac{8}{21} \\cdot 0.5435$\n",
    "\n",
    "        * $H(Y|X) = 0.207$\n",
    "\n",
    "##### Conditional Entropy for when $X_1$ = F\n",
    "\n",
    "* $H(Y|X=X_1=F) =(-\\dfrac{5}{13}\\cdot log_{2}\\dfrac{5}{13}) - (\\dfrac{8}{13}\\cdot log_{2}\\dfrac{8}{13})$\n",
    "\n",
    "    * $(-0.3845 \\cdot -1.3785) - (0.6153 \\cdot -0.7004)$\n",
    "\n",
    "    * $0.5300 - -0.4309$\n",
    "\n",
    "    * $H(Y|X=X_1=F) = 0.9609$\n",
    "\n",
    "    * $H(Y|X) = P(X=X_1=F) \\cdot H(Y|X=X_1=F)$\n",
    "\n",
    "        * $\\dfrac{13}{21} \\cdot 0.9609$\n",
    "\n",
    "        * $H(Y|X) = 0.5948$\n",
    "\n",
    "The formula for information gain is $H(Y) - H(Y|X)$\n",
    "\n",
    "*   $0.985 - (0.207 + 0.5948) = 0.1832$\n",
    "\n",
    "The information gain when we split on $X_1$ is 0.18 bits.\n",
    "\n",
    "___\n",
    "\n",
    "#### Information gain for $X_2$:\n",
    "##### Conditional Entropy for when $X_2$ = T\n",
    "\n",
    "* $H(Y|X=X_2=T) = (-\\dfrac{7}{10}\\cdot log_{2}\\dfrac{7}{10}) - (\\dfrac{3}{10}\\cdot log_{2}\\dfrac{3}{10})$\n",
    "\n",
    "    * $(-0.7 \\cdot -0.5146) - (0.3 \\cdot -1.737)$\n",
    "\n",
    "    * $0.360 - -0.5211$\n",
    "\n",
    "    * $H(Y|X=X_2=T) = 0.8813$\n",
    "\n",
    "    * $H(Y|X) = P(X=X_2=T) \\cdot H(Y|X=X_2=T)$\n",
    "\n",
    "        * $\\dfrac{10}{21} \\cdot 0.8813$\n",
    "\n",
    "        * $H(Y|X) = 0.4196$\n",
    "\n",
    "##### Conditional Entropy for when $X_2$ = F\n",
    "\n",
    "* $H(Y|X=X_2=F) = (-\\dfrac{5}{11}\\cdot log_{2}\\dfrac{5}{11}) - (\\dfrac{6}{11}\\cdot log_{2}\\dfrac{6}{11})$\n",
    "    \n",
    "    * $(-\\dfrac{5}{11}\\cdot-1.1375)-(\\dfrac{6}{11}\\cdot-0.8745)\\ $\n",
    "\n",
    "    * $0.5169 - -0.477$\n",
    "\n",
    "    * $H(Y|X=X_2=F) = 0.9940$\n",
    "\n",
    "    * $H(Y|X) = P(X=X_2=F) \\cdot H(Y|X=X_2=F)$\n",
    "\n",
    "        * $\\dfrac{11}{21} \\cdot 0.9940$\n",
    "\n",
    "        * $H(Y|X) = 0.5206$\n",
    "\n",
    "The formula for information gain is $H(Y) - H(Y|X)$\n",
    "\n",
    "*   $0.985 - (0.4196 + 0.5206) = 0.0448$\n",
    "\n",
    "The information gain when we split on $X_2$ is 0.04 bits.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "$X_1$ information gain is 0.18 and $X_2$ information gain is 0.04. We should split on $X_1$ first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4:  log odds ratios (44 points)\n",
    "This exercise is an exploratory analysis of the Sentiment140 dataset. Sentiment140 combines 160K tweets collected via the Twitter API with most of the emoticons removed. Each tweet is annotated with polarity: positive (4), negative (0) and neutral (2). You do not have to check the original paper that proposed this dataset, but if you are curious, here is the link: [https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "In this problem, we will analyze how often a word tend to appear with a positive sentiment vs. a negative one. The metric we are going to use is  **log odds ratio**, that compares the conditional probability of a word occurring in one type of sentences, say, positive ($P(word|pos)$), and the word occurring in another type of sentences, say, negative ($P(word|neg)$):\n",
    "$$log\\_odds\\_ratio(word, pos) = \\log\\frac{P(word|pos)}{P(word|neg)}$$\n",
    "The higher the $log\\_odds\\_ratio$, the more likely the word is associated with positive sentences.\n",
    "\n",
    "We will use the Sentiment140 dataset. Sentiment140 combines 1.6 million tweets collected via the Twitter API with most of the emoticons removed. Each tweet is annotated with polarity: positive (4), negative (0) or neutral (2). _We will  **not** consider neutral tweets in this problem_. You do not have to check the original paper that proposed this dataset, but if you are curious, here is the link: [https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "Download from Canvas the file ``sentiment140_sample1.csv`` ---a 10K sample from the training set of Sentiment140---and put it under the  **same directory** (folder) as your python script or notebook file. As a reminder, the file is formatted under six fields, including polarity, tweet ID, date, query username and the text of the tweet. We will only use polarity and tweet text in this assignment.\n",
    "\n",
    "In the following exercises, we have provided several expected inputs and outputs of the functions that you will implement. Treat these as test cases for your code; if you get numbers very far off from what is listed here with the same input, you have bugs to crush."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1752285545</td>\n",
       "      <td>Sat May 09 21:30:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Doomed_Vampire</td>\n",
       "      <td>Doesn't know what to eat! What would you eat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1468206560</td>\n",
       "      <td>Tue Apr 07 00:17:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>rynequin</td>\n",
       "      <td>Doesn't know why, but is feeling very down. An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2179666845</td>\n",
       "      <td>Mon Jun 15 09:25:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Deeluvly</td>\n",
       "      <td>@OfficialTG3 you know I'm leaving Texas, right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2219598618</td>\n",
       "      <td>Thu Jun 18 00:54:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>katyshepherdx</td>\n",
       "      <td>I really hate her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1822816157</td>\n",
       "      <td>Sat May 16 20:27:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jennyconfetti</td>\n",
       "      <td>No one's replying to my texts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3               4  \\\n",
       "0  0  1752285545  Sat May 09 21:30:51 PDT 2009  NO_QUERY  Doomed_Vampire   \n",
       "1  0  1468206560  Tue Apr 07 00:17:43 PDT 2009  NO_QUERY        rynequin   \n",
       "2  0  2179666845  Mon Jun 15 09:25:05 PDT 2009  NO_QUERY        Deeluvly   \n",
       "3  0  2219598618  Thu Jun 18 00:54:58 PDT 2009  NO_QUERY   katyshepherdx   \n",
       "4  0  1822816157  Sat May 16 20:27:14 PDT 2009  NO_QUERY   jennyconfetti   \n",
       "\n",
       "                                                   5  \n",
       "0   Doesn't know what to eat! What would you eat ...  \n",
       "1  Doesn't know why, but is feeling very down. An...  \n",
       "2   @OfficialTG3 you know I'm leaving Texas, right?   \n",
       "3                               I really hate her.    \n",
       "4                     No one's replying to my texts   "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "import pandas\n",
    "sentiment_data = pandas.read_csv(\"sentiment140_sample1.csv\", header = None, encoding = \"ISO-8859-1\")\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.984400</td>\n",
       "      <td>2.001648e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000039</td>\n",
       "      <td>1.921348e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467817e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.957661e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.002835e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.177586e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.329052e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1\n",
       "count  10000.000000  1.000000e+04\n",
       "mean       1.984400  2.001648e+09\n",
       "std        2.000039  1.921348e+08\n",
       "min        0.000000  1.467817e+09\n",
       "25%        0.000000  1.957661e+09\n",
       "50%        0.000000  2.002835e+09\n",
       "75%        4.000000  2.177586e+09\n",
       "max        4.000000  2.329052e+09"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Frequency counts  (11 points)\n",
    "First, let's create dictionaries that record the count of each word in positive tweets, as well as the count of each word in negative tweets. Here, here, ``counts[\"pos\"]`` will contain key-value pairs of a word and its number of appearance in positive tweets, ``counts[\"neg\"]`` will contain key-value pairs of a word and its number of appearance in negative tweets\n",
    "\n",
    "To parse the tweets, we will use NLTK's ``word_tokenize()`` function. As an example, the following tokenizes a sentence into a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eloraghespie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sentence', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #you only have to do this once per environment\n",
    "\n",
    "from nltk import word_tokenize\n",
    "word_tokenize(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower-casing all words gives cleaner counts. For example, consider the two sentences: \"Apples are delicious. John loves apples.\" If we do not lower-case each word, ''Apples'' and ''apples'' will be counted as two different words. In Python, you can lower-case a word by calling ``lower()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Apples becomes apples\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"Apples\" == \"apples\")\n",
    "print(\"Apples becomes\", \"Apples\".lower())\n",
    "print(\"Apples\".lower() == \"apples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider words and not symbols or numbers. To test whether a word is a word, that is, consisting of only English characters, we can use ``isalpha()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"Apples\".isalpha())\n",
    "print(\"Apples123\".isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "31\n",
      "17\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "def get_counts(data):\n",
    "    \"\"\" \n",
    "    counts the number of times a word appears in negative or positive tweets\n",
    "    \n",
    "    Parameters:\n",
    "    data: Pandas dataframe of tweets\n",
    "    \n",
    "    Returns:\n",
    "    counts: Dictionary of counts, which includes the dictionaries 'pos' and 'neg'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counts = {\"pos\":{}, \"neg\":{}}\n",
    "    \n",
    "    # loop through the rows\n",
    "    for idx, row in sentiment_data.iterrows():\n",
    "\n",
    "        # lowercase and tokenize each tweet\n",
    "        data = word_tokenize(row[5].lower())\n",
    "\n",
    "        # check for negative tweets\n",
    "        if row[0] == 0:\n",
    "            for word in data:\n",
    "                # only mark the alpha characters\n",
    "                if word.isalpha():\n",
    "                    # add to dictionary\n",
    "                    if word in counts['neg']:\n",
    "                        counts['neg'][word] += 1\n",
    "                    else:\n",
    "                        counts['neg'][word] = 1\n",
    "\n",
    "        # check for positive tweets\n",
    "        elif row[0] == 4:\n",
    "            for word in data:\n",
    "                # only mark the alpha characters\n",
    "                if word.isalpha():\n",
    "                    # add to dictionary\n",
    "                    if word in counts['pos']:\n",
    "                        counts['pos'][word] += 1\n",
    "                    else:\n",
    "                        counts['pos'][word] = 1\n",
    "\n",
    "    \n",
    "    return counts\n",
    "\n",
    "    \n",
    "# Do not change\n",
    "counts = get_counts(sentiment_data)\n",
    "\n",
    "print(counts[\"pos\"][\"happy\"]) # should print 115\n",
    "print(counts[\"neg\"][\"happy\"]) # should print 31\n",
    "print(counts[\"pos\"][\"hate\"]) # shuld print 17\n",
    "print(counts[\"neg\"][\"hate\"]) # should print 106\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Calculating $P(\\text{word}|\\text{polarity})$ (11 points)\n",
    "\n",
    "Create a function ``get_word_prob(counts, word, polarity)``, where ``counts`` is a dictionary like in the previous task, ``word`` is the word for which $P(word|polarity)$ will be calculated, and ``polarity`` is either ``pos`` or ``neg``. The function should return $P(word|polarity)$. If ``counts[polarity]`` does not contain ``word``, then return 0.\n",
    "\n",
    "Note that you should NOT need to use the variable ``data`` here, and only rely on the three arguments of the function: ``counts, word, polarity``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002390373899701203\n",
      "0.00025490313680801295\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def get_word_prob(counts, word, polarity):\n",
    "    \"\"\" \n",
    "    calculates the probability of a word given a polarity \n",
    "    \n",
    "    Parameters:\n",
    "    counts (dict): the dictionaries 'pos' and 'neg' which count word occurances\n",
    "    word (str): the word you want to get the probability for\n",
    "    polarity (str): wither 'pos' or 'neg'\n",
    "    \n",
    "    Returns:\n",
    "    probability (float):  the probability of a word given a polarity \n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "\n",
    "    # number of times a word appears in the given polarity dict\n",
    "    # 0 if it's not there at all\n",
    "    count = counts[polarity][word] if word in counts[polarity] else 0\n",
    "\n",
    "    # the number of word occurences all summed within the given polarity dict\n",
    "    pol = sum([value for value in counts[polarity].values()])\n",
    "\n",
    "    probability = count/pol\n",
    "    \n",
    "    return probability # P(word|polarity)\n",
    "\n",
    "#Do not change\n",
    "print(get_word_prob(counts, \"great\", \"pos\")) # should be ~0.00239\n",
    "print(get_word_prob(counts, \"glad\", \"neg\")) # should be ~0.000255\n",
    "print(get_word_prob(counts, \"wugs\", \"neg\")) # should be 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Calculate the log odds ratio of a word  (11 points)\n",
    "\n",
    "\n",
    "Using the above function, we can calculate $P(word|pos)$ and $P(word|neg)$ given a word, so we are ready to calculate the log odds ratio of that word as well. Create a function ``log_odds_ratio(count_dict, word, polarity)``, where the arguments are of the same type/format as in the previous problem. The function should return $log\\_odds\\_ratio(word)$:\n",
    "\n",
    "$$ log\\_odds\\_ratio(word, polarity) = \\log\\frac{P(word|polarity)}{P(word|opposite\\_polarity)} $$\n",
    "\n",
    "If the denominator is zero, return a very large number (please return 10000). Again you should NOT need to use the variable ``data`` here, and only rely on the three arguments of the function: ``counts``, ``word``, and ``polarity``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2873451688770043\n",
      "-0.0779544942827149\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "def log_odds_ratio(counts, word, polarity):\n",
    "    \"\"\" \n",
    "    This function returns the log odds ratio of a term (see previous cell)\n",
    "    \n",
    "    Parameters:\n",
    "    counts (dict): the dictionaries 'pos' and 'neg' which count word occurances\n",
    "    word (str): the word you want to get the probability for\n",
    "    polarity (str): wither 'pos' or 'neg'\n",
    "    \n",
    "    Returns:\n",
    "    log_odds_ratio (float): log( prob(word|plarity) / P(word|opposite_polarity) )\n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "\n",
    "    import math\n",
    "\n",
    "    # identify what the opposite polarity is\n",
    "    if polarity == 'pos':\n",
    "        opposite_polarity = 'neg'\n",
    "    else:\n",
    "        opposite_polarity = 'pos'\n",
    "\n",
    "    # calculate the log odds ratio\n",
    "    try:    \n",
    "        log_odds_ratio = math.log(get_word_prob(counts, word, polarity) / get_word_prob(counts, word, opposite_polarity))\n",
    "\n",
    "    # if an error is raised, likely a ZeroDivision Error, return the number \n",
    "    except:\n",
    "        log_odds_ratio = 10000\n",
    "    \n",
    "    return log_odds_ratio\n",
    "\n",
    "\n",
    "# Do not change\n",
    "print(log_odds_ratio(counts, \"great\", \"pos\")) # should be ~1.287\n",
    "print(log_odds_ratio(counts, \"the\", \"neg\")) #  should be ~-0.0779\n",
    "print(log_odds_ratio(counts, \"wug\", \"neg\")) # should be a very large number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Sorting log odds ratios (11 points)\n",
    "\n",
    "After being able to calculate log odds ratios for individual words, we can now sort words according to its association with a polarity class, say, positive. Create a function ``sort_pos_words(data)``, that takes in the entire dataframe as an argument, and return a sorted list of ``(word, log odds ratio)`` tuples for the positive sentiment class.\n",
    "\n",
    "If you implement this without filtering out any words, you will notice that there are many cases where the conditional probability of the denominator is 0, leading to the very large number you specified in the ``log_odds_ratio()`` function. This is because most words appear only once in the dataset. One way to mitigate this issue is to consider only words that appeared at least $x$ times in the dataset; here, let's only include words that appeared more than 10 times in the dataset, regardless of the polarity of the tweet (positive or negative).\n",
    "\n",
    "Use your function to print out the top 10 most positive (Note: we only consider those words appear in the positive sentiment class, namely you only need to sort the words in positive sentiment class and take the top-10 and bottom-10):\n",
    "\n",
    "`` [('followfriday', 10000), ('ff', 10000), ('proud', 10000), ('yummy', 3.1187445986382114), ('congrats', 3.0699544344687792), ('moon', 2.713279490530047), ('exciting', 2.639171518376325), ('gorgeous', 2.5591288107027883), ('welcome', 2.5165691962839927), ('prom', 2.4721174337131586)]``\n",
    "       \n",
    " and the top 10  most negative:\n",
    " \n",
    "`` [('bummed', -2.410684488873212), ('scared', -2.410684488873212), ('stomach', -2.4907271965467483), ('ugh', -2.5648351687004705), ('happened', -2.8702168182516523), ('worse', -2.9215101126392025), ('throat', -2.9703002768086346), ('died', -3.144653663953412), ('sad', -3.5203466137279067), ('hurts', -3.589339485214858)]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most positive \n",
      " [('followfriday', 10000), ('ff', 10000), ('proud', 10000), ('yummy', 3.1188449667545735), ('congrats', 3.0700548025851417), ('moon', 2.7133798586464093), ('exciting', 2.639271886492687), ('gorgeous', 2.559229178819151), ('welcome', 2.5166695644003547), ('prom', 2.472217801829521)]\n",
      "Top 10 most negative \n",
      " [('bummed', -2.41058412075685), ('scared', -2.41058412075685), ('stomach', -2.4906268284303863), ('ugh', -2.564734800584108), ('happened', -2.87011645013529), ('worse', -2.9214097445228404), ('throat', -2.9701999086922726), ('died', -3.14455329583705), ('sad', -3.5202462456115446), ('hurts', -3.589239117098496)]\n"
     ]
    }
   ],
   "source": [
    "def sort_pos_words(data):\n",
    "    \"\"\"\n",
    "    takes in a pandas dataframe and outputs the top 10 most positive and negative words in the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): the tweets in a dataframe\n",
    "    \n",
    "    Return:\n",
    "    sorted_list (list): a sorted list of (word, log odds ratio) tuples for the positive sentiment class\n",
    "    \n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    \n",
    "    sorted_list = []\n",
    "    counts = get_counts(data)\n",
    "\n",
    "    # we are only concerned with words that are in pos\n",
    "    for word in counts['pos']:\n",
    "        # if it occurs more than 10 times across the entire dataset\n",
    "        if counts['pos'][word] + (counts['neg'][word] if word in counts['neg'] else 0) > 10:\n",
    "                # take the log odds ratio and put it in the list\n",
    "                sorted_list.append((word, log_odds_ratio(counts, word,'pos')))\n",
    "    \n",
    "    # sort the list based on log odds ratio\n",
    "    sorted_list = sorted(sorted_list, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_list\n",
    "    \n",
    "# Do not change\n",
    "lst = sort_pos_words(sentiment_data)\n",
    "print(\"Top 10 most positive \\n\", lst[:10]) # see previous cell for what this should print\n",
    "print(\"Top 10 most negative \\n\", lst[-10:])    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
