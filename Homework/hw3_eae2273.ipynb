{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 371 Machine Learning for Text Analysis\n",
    "\n",
    "# Homework 3 - due Monday March 4 2024 at 11:59pm\n",
    "\n",
    "For this homework you will hand in (upload) to Canvas a notebook renamed ``hw3_YourEID.ipynb``.\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) an ensure your code outputs the correct answer.\n",
    "\n",
    "A perfect solution for this homework is worth **100** points. For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors.Â **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! You can create posts on Ed Discussions to look for group partners :)\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jessyli.com/courses/lin371\n",
    "\n",
    "For typing up your answers to non-programming problems, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any collaborators here:\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Logistic Regression\n",
    "\n",
    "* **(a)** (5 points) In logistic regression, we know that when there are two classes an example can belong to ($y\\in\\{0,1\\}$), for each example ${\\bf x}$ with M features,\n",
    "    $$\n",
    "    p(y=1|x_1, x_2, ..., x_M) = \\frac{1}{1+\\exp{ (-\\sum_{j=1}^M w_jx_j-b) }}\n",
    "    $$\n",
    "    $$\n",
    "    p(y=0|x_1, x_2, ..., x_M) = 1-p(y=1|x_1, x_2, ..., x_M)\n",
    "    $$\n",
    "How many parameters does logistic regression estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It estimates 1. It estimates whether y = 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b)** (10 points) Briefly describe, given a dataset of $N$ examples, how are these parameters estimated. You do not need to lay out specific mathematical derivations, rather, provide a summary of a few sentences of the process. You can assume that all these examples will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(c)** (10 points) Logistic regression can be extended to classify $K$ classes instead of only 2. When $y$ can be one of K classes, the conditional probability of $y$ being of a particular class $k$ is:\n",
    " $$\n",
    " p(y=k|x_1,x_2,...,x_M) = \\frac{\\exp(\\sum_{j=1}^M w_{jk}x_j+b_k)}{\\sum_{k'=1}^K\\exp(\\sum_{j=1}^M w_{jk'}x_j+b_{k'})}\n",
    " $$\n",
    "How many parameters are we estimating in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Regularization and evaluation\n",
    "\n",
    "Recall that with Ridge Regression, we add a _regularization_ term to the log likelihood:\n",
    "$$\n",
    "    l({\\bf w}) = \\log \\prod_{i=1}^N{ p(y^{(i)}|{\\bf x^{(i)},w})}-\\lambda||{\\bf w}||_2^2\n",
    "$$\n",
    "\n",
    "* **(a)** (10 points) What is the goal of this regularization term? How does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b)** (15 points) Your friend David uses cross validation to pick $\\lambda$ for his application of logistic regression as follows. He sets aside 20\\% of his data as a test set, and then runs cross validation on his training data to get cross validation accuracy with multiple values of $\\lambda$. He also trains a model on the entire training data for each value of $\\lambda$ and computes test accuracy on his held out data. He gets the following results:\n",
    "\n",
    "| $\\lambda$ | Cross-validation,accuracy | Test accuracy |\n",
    "|:---------:|:-------------------------:|:-------------:|\n",
    "|     0     |            0.7            |      0.65     |\n",
    "|     1     |            0.75           |      0.7      |\n",
    "|     10    |            0.8            |      0.63     |\n",
    "|    100    |            0.7            |      0.6      |\n",
    "\n",
    "* **(b-i)** What is the optimal $\\lambda$ in terms of test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b-ii)** Is this the same $\\lambda$ that would be chosen by cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b-iii)** David wants to inform the public about his model and his results. What should he report as the test accuracy of his method (logistic regression with regularization parameter $\\lambda$) on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Logistic Regression in sklearn\n",
    "\n",
    "Logistic regression, it turns out, is a natural \"cousin\" to Naive Bayes; specifically it is the \"discriminative\" variant. First, let's review the code for the sklearn version of [Naive Bayes (sentiment analysis notebook)](https://utexas.instructure.com/courses/1296690/files/folder/06-naivebayes). Note that we define a random state here, so that the answers will be consistent across everyone (it should be 0.8175)! \n",
    "\n",
    "Below is the code from our Naive Bayes (sentiment analysis) notebook which serves as a starting point for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_model accuracy is :  0.8175\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       197\n",
      "           1       0.82      0.82      0.82       203\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## PLEASE RUN THIS CELL BUT DO NOT MODIFY.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# load data using sklearn.datasets.load_files\n",
    "dataset = load_files(\"movie-reviews/\")\n",
    "\n",
    "# split the data into train and test\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size = 0.2, random_state = 3)\n",
    "\n",
    "# vectorize the training data\n",
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "X_train = vectorizer.fit_transform(docs_train)\n",
    "\n",
    "# fit the model with the training data\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# vectorize the test data and predict \n",
    "X_test = vectorizer.transform(docs_test)\n",
    "y_hat_nb = nb_model.predict(X_test)\n",
    "\n",
    "# get accuracy score\n",
    "accuracy_score_nb = metrics.accuracy_score(y_test, y_hat_nb)\n",
    "\n",
    "# print out some data\n",
    "print(\"nb_model accuracy is : \", accuracy_score_nb)\n",
    "print(\"classification report:\\n\", metrics.classification_report(y_test, y_hat_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(accuracy_score_nb == 0.8175)\n",
    "## If this throws and error, pleast contact the TA immediately!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some of the problems below, we have included the expected outputs (e.g., performance of the models) as `assertions` in the cells immediately below the cell where your implementation goes; they start with `## Do not edit this cell - autograder test`.\n",
    "\n",
    "**(a)** (10 points) Train a logistic regression model named `lr_model`, using the [logistic regression model provided by sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). How do `lr_model` and `nb_model` compare in terms of accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/eloraghespie/Documents/GitHub/LIN 371/LIN-371/Homework/hw3_eae2273.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LIN%20371/LIN-371/Homework/hw3_eae2273.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# use random_state = 3, solver='liblinear'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LIN%20371/LIN-371/Homework/hw3_eae2273.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# From documentation\"For small datasets, âliblinearâ is a good choice, whereas âsagâ and âsagaâ are faster for large ones.\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LIN%20371/LIN-371/Homework/hw3_eae2273.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LIN%20371/LIN-371/Homework/hw3_eae2273.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LIN%20371/LIN-371/Homework/hw3_eae2273.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LIN%20371/LIN-371/Homework/hw3_eae2273.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LIN%20371/LIN-371/Homework/hw3_eae2273.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlr_model accuracy is : \u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m.\u001b[39maccuracy_score(y_test, y_hat_lr))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LIN%20371/LIN-371/Homework/hw3_eae2273.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mclassification report:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, metrics\u001b[39m.\u001b[39mclassification_report(y_test, y_hat_lr))\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use random_state = 3, solver='liblinear'\n",
    "# From documentation\"For small datasets, âliblinearâ is a good choice, whereas âsagâ and âsagaâ are faster for large ones.\"\n",
    "# \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"lr_model accuracy is : \", metrics.accuracy_score(y_test, y_hat_lr))\n",
    "print(\"classification report:\\n\", metrics.classification_report(y_test, y_hat_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(lr_model.solver == 'liblinear')\n",
    "assert(lr_model.random_state == 3)\n",
    "assert(accuracy_score_lr == 0.8475)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model is better?\n",
    "\n",
    "# YOUR CODE HERE: change \"None\" to be either \"nb\" or \"lr\"\n",
    "better_basic_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** (10 points) Instead of performing just one train/test split, try performing cross validation using the sklearn function `cross_val_score`. [See here for documentation on doing this](https://scikit-learn.org/stable/modules/cross_validation.html). Using 10-fold cross-validation, retrain and run both NB and LR. For cross validation, because the evaluator will automatically section your data for you, make sure to:\n",
    "* define and train new NB/LR models\n",
    "* fit your (new) vectorizer on the entire dataset\n",
    "* use the entire dataset in the socring function `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use random_state = 3 and solver='liblinear'\n",
    "# For cross validation, make sure you are using the entire dataset as your \n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "\n",
    "print(\"Accuracy for Logistic Regression: %0.4f (+/- %0.4f)\" % (lr_xval_scores.mean(), lr_xval_scores.std() * 2))\n",
    "print(\"Accuracy for Naive Bayes: %0.4f (+/- %0.4f)\" % (nb_xval_scores.mean(), nb_xval_scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(lr_xval_model.solver == 'liblinear')\n",
    "assert(lr_xval_model.random_state == 3)\n",
    "assert(round(lr_xval_scores.mean(),4) == 0.8470)\n",
    "assert(round(nb_xval_scores.mean(), 4) == 0.8095)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which model is better?\n",
    "\n",
    "# YOUR CODE HERE: change \"None\" to be either \"nb\" or \"lr\"\n",
    "better_cv_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** (5 points) Now, let's modify the features in our classifier! The first thing we're trying is to have our classifier not only include unigrams, but also bigrams. Modify your `CountVectorizer` to include bigrams and use logistic regression as your classifier. Did the results on 10-fold cross validation improve? (Make sure to use the same folds as before, so the results are comparable!)\n",
    "\n",
    "*Hint*: take a look at the `CountVectorizer` documentation -- you only need to change one parameter in this function to enable bigrams! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Accuracy for Logistic Regression: %0.4f (+/- %0.4f)\" % (lr_bigram_scores.mean(), lr_bigram_scores.std() * 2))\n",
    "print(\"Accuracy for Naive Bayes: %0.4f (+/- %0.4f)\" % (nb_bigram_scores.mean(), nb_bigram_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(lr_bigram_model.solver == 'liblinear')\n",
    "assert(lr_bigram_model.random_state == 3)\n",
    "assert(round(lr_bigram_scores.mean(),4) == 0.8500)\n",
    "assert(round(nb_bigram_scores.mean(), 4) == 0.8085)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** (25 points) Let's also add some features. Specifically, we will add the count of part-of-speech (POS): for a particular document, how many nouns are there? How many verbs?... and so on.\n",
    "\n",
    "* **(d-i)** (7 points) write a function `get_pos_tags(doc)` that when given a document, returns the list of POS tags. Use NLTK for this.\n",
    "```\n",
    ">>> get_pos_tags(\"This is a test case with a two singular nouns.\")\n",
    "Counter({'DT': 3, 'NN': 2, 'VBZ': 1, 'IN': 1, 'CD': 1, 'JJ': 1, 'NNS': 1, '.': 1})\n",
    ">>> get_pos_tags(\"This is a test case with multiple sentences. Will you get it right?\")\n",
    "Counter({'DT': 2, 'NN': 2, '.': 2, 'PRP': 2, 'VBZ': 1, 'IN': 1, 'JJ': 1, 'NNS': 1, 'MD': 1, 'VB': 1, 'RB': 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "def get_pos_tags(text) -> Counter:\n",
    "    \"\"\" when given a string, returns a POS tag counter, using NLTK\"\"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "print(get_pos_tags(\"This is a test case with a two singular nouns.\"))\n",
    "print(get_pos_tags(\"This is a test case with multiple sentences. Will you get it right?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not edit this cell - autograder test\n",
    "assert(get_pos_tags(\"This is a test case with a two singular nouns.\")[\"NN\"] == 2)\n",
    "assert(get_pos_tags(\"This is a test case with multiple sentences. Will you get it right?\")[\"MD\"] == 1)\n",
    "assert(get_pos_tags(str(dataset.data[0]))[\"JJ\"] == 44)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_pos_features` in the cell below creates a DataFrame for the POS tags; you can visualize the outcome from the `print()` line at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not modify this cell, but do execute it.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_pos_features(docs: list) -> pd.DataFrame:\n",
    "    \n",
    "    _temp_df = pd.DataFrame(docs, columns=[\"docs\"])\n",
    "    _temp_df = _temp_df.docs.apply(get_pos_tags).to_list()\n",
    "    _df_pos = pd.DataFrame(_temp_df).fillna(0)\n",
    "    return _df_pos\n",
    "\n",
    "print(get_pos_features(dataset.data[:2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(d-ii)** (7 points) Now, write a function `transform_data` and use ColumnTransformer to combine *unigram* and POS features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "import scipy\n",
    "\n",
    "def transform_data(raw_data) -> pd.DataFrame:        \n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, using ColumnTransformer will return a numpy array with a smaller amount of data, \n",
    "# but a scipi sparse matrix with more data.  \n",
    "\n",
    "## Do not edit this cell - autograder test\n",
    "assert(type(transform_data(dataset.data[:3])) == np.ndarray)\n",
    "assert(transform_data(dataset.data[:3])[0][5] == 1)\n",
    "assert(type(transform_data(dataset.data[:20])) == scipy.sparse.csr.csr_matrix)\n",
    "print(\"visable test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(d-iii)** (6 points) Finally, re-train your logistic regression model over the combined features (run transform_data on the entire dataset first). What is the 10-fold cross validation accuracy? (Make sure to use the same folds as before, so the results are comparable! Feature extraction should finish in under 10 mins for the entire dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use random_state = 3\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Accuracy for Logistic Regression: %0.4f (+/- %0.4f)\" % (lr_xval_pos_scores.mean(), lr_xval_pos_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(d-v)** (5 points) Some analysis: essentially, we have trained two models: LR with unigrams and bigrams, and LR with unigrams and POS tags. This setting allows us to compare the power of the new features (bigrams/POS tags). Which one is the better feature for the sentiment analysis task? Explain at least one linguistic reason why this may be true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
