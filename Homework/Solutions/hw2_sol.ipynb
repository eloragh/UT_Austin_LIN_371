{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-737ecffa804edefe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# LIN 371 Machine Learning for Text Analysis\n",
    "\n",
    "# Homework 2 - due Friday Feb 16 2024 at 11:59pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas your code named ``hw2_YourEID.ipynb``.\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) and ensure your code outputs the correct answer.\n",
    "\n",
    "A perfect solution for this homework is worth **100** points, and there is a bonus problem at the end that is worth **5 points**. For non-programming tasks, you must show work to get partial credit. For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors. **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! There is a channel on Slack where you can look for a study group.\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jessyli.com/courses/lin371\n",
    "\n",
    "For typing up your answers to problems 1, 2 and 3, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any collaborators here:\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e27014b61dce338",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 1: Naive Bayes\n",
    "\n",
    "We'll do a bit of manual parameter estimation here.\n",
    "\n",
    "## **(a)** (8 points) \n",
    "Calculate the sufficient parameters for Naive Bayes using the data in the figure below, that\n",
    "is, the prior class probabilities and the conditional probabilities for all of the symbols.\n",
    "\n",
    "doc | X | Y\n",
    "-- | --| --\n",
    "d1 | a b b b c b | +\n",
    "d2 | c a c c c b | -\n",
    "d3 | c c c b | -\n",
    "d4 | b a b b b | +\n",
    "d5 | b c a b b | ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution 1: with Laplace Smoothing\n",
      "----------------------------------\n",
      "p(+) = 0.5 \t \t \t p(-) = 0.5\n",
      "p(a|+) = 0.21428571428571427 \t p(a|-) = 0.15384615384615385\n",
      "p(b|+) = 0.6428571428571429 \t p(b|-) = 0.23076923076923078\n",
      "p(a|+) = 0.21428571428571427 \t p(a|-) = 0.6153846153846154\n",
      "\n",
      "==================================\n",
      "\n",
      "Solution 2: w/o Laplace Smoothing\n",
      "----------------------------------\n",
      "p(+) = 0.5 \t \t \t p(-) = 0.5\n",
      "p(a|+) = 0.18181818181818182 \t p(a|-) = 0.1\n",
      "p(b|+) = 0.7272727272727273 \t p(b|-) = 0.2\n",
      "p(a|+) = 0.18181818181818182 \t p(a|-) = 0.7\n"
     ]
    }
   ],
   "source": [
    "## BEGIN SOLUTION\n",
    "## With Laplace Smoothing\n",
    "print (\"Solution 1: with Laplace Smoothing\")\n",
    "print (\"----------------------------------\")\n",
    "P_pos = 1/2\n",
    "P_neg = 1/2\n",
    "              \n",
    "Pa_pos = 3/14\n",
    "Pb_pos = 9/14\n",
    "Pc_pos = 2/14\n",
    "\n",
    "Pa_neg = 2/13\n",
    "Pb_neg = 3/13\n",
    "Pc_neg = 8/13\n",
    "\n",
    "## END SOLUTION\n",
    "\n",
    "print(f\"p(+) = {P_pos} \\t \\t \\t p(-) = {P_neg}\")\n",
    "print(f\"p(a|+) = {Pa_pos} \\t p(a|-) = {Pa_neg}\")\n",
    "print(f\"p(b|+) = {Pb_pos} \\t p(b|-) = {Pb_neg}\")\n",
    "print(f\"p(a|+) = {Pa_pos} \\t p(a|-) = {Pc_neg}\")\n",
    "\n",
    "## BEGIN HIDDEN TESTS\n",
    "assert(P_pos == 1/2)\n",
    "assert(P_neg == 1/2)\n",
    "              \n",
    "assert(Pa_pos == 3/14)\n",
    "assert(Pb_pos == 9/14)\n",
    "assert(Pc_pos == 2/14)\n",
    "\n",
    "assert(Pa_neg == 2/13)\n",
    "assert(Pb_neg == 3/13)\n",
    "assert(Pc_neg == 8/13)\n",
    "\n",
    "## END HIDDEN TESTS\n",
    "\n",
    "# W/o Laplace Smoothing\n",
    "print (\"\\n==================================\\n\")\n",
    "print (\"Solution 2: w/o Laplace Smoothing\")\n",
    "print (\"----------------------------------\")\n",
    "P_pos_wo = 1/2\n",
    "P_neg_wo = 1/2\n",
    "              \n",
    "Pa_pos_wo = 2/11\n",
    "Pb_pos_wo = 8/11\n",
    "Pc_pos_wo = 1/11\n",
    "\n",
    "Pa_neg_wo = 1/10\n",
    "Pb_neg_wo = 1/5\n",
    "Pc_neg_wo = 7/10\n",
    "\n",
    "print(f\"p(+) = {P_pos_wo} \\t \\t \\t p(-) = {P_neg_wo}\")\n",
    "print(f\"p(a|+) = {Pa_pos_wo} \\t p(a|-) = {Pa_neg_wo}\")\n",
    "print(f\"p(b|+) = {Pb_pos_wo} \\t p(b|-) = {Pb_neg_wo}\")\n",
    "print(f\"p(a|+) = {Pa_pos_wo} \\t p(a|-) = {Pc_neg_wo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(b)** (7 points) \n",
    "Using these, calculate the most likely class (pos or neg) for the unlabeled example (d5, labeled ???)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution 1: with Laplace Smoothing\n",
      "----------------------------------\n",
      "P(+|d5) = 0.004066386029630512\n",
      "P(-|d5) = 0.0005817508005806736\n",
      "The more likely class is pos\n",
      "\n",
      "==================================\n",
      "\n",
      "Solution 2: w/o Laplace Smoothing\n",
      "----------------------------------\n",
      "P(+|d5) = 0.0031791171740628748\n",
      "P(-|d5) = 0.00028000000000000003\n",
      "The more likely class is pos\n"
     ]
    }
   ],
   "source": [
    "print (\"Solution 1: with Laplace Smoothing\")\n",
    "print (\"----------------------------------\")\n",
    "more_likely_class = \"set to 'neg' or 'pos'\"\n",
    "\n",
    "P_pos_d5 = P_pos * (Pb_pos**3) * Pc_pos * Pa_pos\n",
    "P_neg_d5 = P_neg * (Pb_neg**3) * Pc_neg * Pa_neg\n",
    "if P_pos_d5 > P_neg_d5:\n",
    "    more_likely_class = 'pos'\n",
    "else: \n",
    "    more_likely_class = 'neg'\n",
    "\n",
    "\n",
    "print(\"P(+|d5) = \" + str(P_pos_d5))\n",
    "print(\"P(-|d5) = \" + str(P_neg_d5))\n",
    "print(\"The more likely class is \" + str(more_likely_class))\n",
    "\n",
    "print (\"\\n==================================\\n\")\n",
    "print (\"Solution 2: w/o Laplace Smoothing\")\n",
    "print (\"----------------------------------\")\n",
    "more_likely_class = \"set to 'neg' or 'pos'\"\n",
    "\n",
    "P_pos_d5 = P_pos_wo * (Pb_pos_wo**3) * Pc_pos_wo * Pa_pos_wo\n",
    "P_neg_d5 = P_neg_wo * (Pb_neg_wo**3) * Pc_neg_wo * Pa_neg_wo\n",
    "if P_pos_d5 > P_neg_d5:\n",
    "    more_likely_class = 'pos'\n",
    "else: \n",
    "    more_likely_class = 'neg'\n",
    "\n",
    "\n",
    "print(\"P(+|d5) = \" + str(P_pos_d5))\n",
    "print(\"P(-|d5) = \" + str(P_neg_d5))\n",
    "print(\"The more likely class is \" + str(more_likely_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Language Modeling\n",
    "\n",
    "In this problem, we will implement our own Shakespeare and Austen language models!\n",
    "\n",
    "### Data\n",
    "\n",
    "We will be working with a few different files under the `data` subdirectory. \n",
    "- `shakespeare.txt`\n",
    "- `shakespeare_short.txt`\n",
    "- `austen.txt`\n",
    "- `austen_short.txt`\n",
    "- `test_short.txt`\n",
    "\n",
    "You can use the \"short\" text files to do testing and debugging as the other files will take a little while (but should be under 10 mins for a 8G RAM laptop) to run! \n",
    "\n",
    "### Note re NLTK\n",
    "\n",
    "The goal of this problem is to think about how to implement language models from first principles. Thus, **the *ONLY* allowable functions from NTLK in this problem are `sent_tokenize` and `word_tokenize`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Clean text. (5 points) \n",
    "\n",
    "Create a function `sent_transform(sent_string)`, such that when given a sentence as a string, it would return a `list` of words, lower-cased. Use NLTK's `word_tokenize` function to tokenize the sentence. For now, you can use any random string to test this function.\n",
    "\n",
    "```\n",
    ">>> sent_transform(\"ROSALIND. Why, whither shall we go?\")\n",
    "['rosalind', '.', 'why', ',', 'whither', 'shall', 'we', 'go', '?']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rosalind', '.', 'why', ',', 'whither', 'shall', 'we', 'go', '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def sent_transform(sent_string):\n",
    "    return [x.lower() for x in word_tokenize(sent_string)]\n",
    "\n",
    "sent_transform(\"ROSALIND. Why, whither shall we go?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Find n-grams. (10 points) \n",
    "\n",
    "Create a function `make_ngram_tuples(words, n)` that returns a sequence of all n-grams seen in the input, in order.  The function returns a sequence of tuples where each tuple is of the form `(context, word)`.  The context is a tuple of the preceding n−1 words for each word; if the number of preceding words is smaller than n−1, place a `<s>` symbol in place of each missing context.  Close each sentence with an additional token `</s>`.  You can assumen n>1, that is, we are interested in enumerating bigrams, trigrams etc, and not unigrams.\n",
    "\n",
    "For now, you can use any random string to test this function.\n",
    "\n",
    "```\n",
    ">>> samples = [’she’, ’eats’, ’happily’ ’.’]\n",
    ">>> make_ngram_tuples(samples, 2)\n",
    "[((’<s>’,), ’she’), ((’she’,), ’eats’), ((’eats’,), ’happily’), ((’happily’,), ’.’), ((’.’,), ’</s>’)]\n",
    ">>> make_ngram_tuples(samples, 3)\n",
    "\n",
    "[((’<s>’, ’<s>’), ’she’), ((’<s>’, ’she’), ’eats’), ((’she’, ’eats’), ’happily’),((’eats’, ’happily’), ’.’), ((’happily’, ’.’), ’</s>’)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('<s>',), 'she'),\n",
       " (('she',), 'eats'),\n",
       " (('eats',), 'happily'),\n",
       " (('happily',), '</s>')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_ngram_tuples(words, n):\n",
    "    tokens = list(words)\n",
    "    to_return = []\n",
    "\n",
    "    for _ in range(n-1):\n",
    "        tokens.insert(0, '<s>')\n",
    "    tokens.append('</s>')\n",
    "    \n",
    "    for idx, token in enumerate(tokens[n-1:]):\n",
    "        if idx < len(tokens):\n",
    "            to_return.append((tuple(tokens[idx:idx+n-1]), token))\n",
    "\n",
    "    return to_return\n",
    "\n",
    "make_ngram_tuples(['she', 'eats', 'happily'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)  Building an n-gram language model\n",
    "\n",
    "We are now ready to estimate a bigram language model from the training data.  We will use **Laplace smoothing (add-1)**.\n",
    "\n",
    "### (c1) Process the training file (8 points)\n",
    "\n",
    "We will first need to transform a file of plain text into a list of \"sentences\", where each \"sentence\" is a list of *lower-cased* words. Implement a function `process_text(textfile)` to do so. Assume that the path of `textfile` is of a form like `data/austen_short.txt`, i.e., relative paths. Make sure that you call `sent_transform` in this function.\n",
    "\n",
    "```\n",
    ">>> processed_sents = process_text(\"data/austen_short.txt\")\n",
    ">>> print(processed_sents[10])\n",
    "['he', 'considered', 'the', 'blessing', 'of', 'beauty', 'as', 'inferior', 'only', 'to', 'the', 'blessing', 'of', 'a', 'baronetcy', ';', 'and', 'the', 'sir', 'walter', 'elliot', ',', 'who', 'united', 'these', 'gifts', ',', 'was', 'the', 'constant', 'object', 'of', 'his', 'warmest', 'respect', 'and', 'devotion', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'considered', 'the', 'blessing', 'of', 'beauty', 'as', 'inferior', 'only', 'to', 'the', 'blessing', 'of', 'a', 'baronetcy', ';', 'and', 'the', 'sir', 'walter', 'elliot', ',', 'who', 'united', 'these', 'gifts', ',', 'was', 'the', 'constant', 'object', 'of', 'his', 'warmest', 'respect', 'and', 'devotion', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "def process_text(textfile):\n",
    "    fullstring = open(textfile).read()\n",
    "    ret = []\n",
    "    for sent in sent_tokenize(fullstring):\n",
    "        ret.append(sent_transform(sent))\n",
    "    return ret\n",
    "\n",
    "processed_sents = process_text(\"data/austen_short.txt\")\n",
    "print(processed_sents[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c2) Vocabulary (8 points).\n",
    "\n",
    "The language model ought to be able to handle words not seen in the training data.  Such words will most certainly appear if the LM is used in some application to estimate the likelihood of new data.   There  are  many  ways  to  incorporate  unknown  vocabulary  in  a  language  model.   In  this problem, we will take all words that appear only once and replace them with the symbol `<UNK>`. Thus, at test time, if we encounter an out-of-vocabulary word, we can resort to replacing the word with `<UNK>` which has well-formed probabilities.\n",
    "\n",
    "First, implement a function `get_vocab(tokenized_sents)` where the parameter `tokenized_sents` is a list of tokenized \"sentences\" (where each \"sentence\" is a list of lower-cased words) returned by the function `process_text`. The function `get_vocab` will return a `set` of all unique words in `tokenized_sents` that appeared more than once.\n",
    "Note: do not forget to add three extra tokens to the vocabulary: `<s>`, `</s>`, and `<UNK>`.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    ">>> vocab = get_vocab(processed_sents)\n",
    ">>> print(len(vocab))\n",
    "302\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocab(tokenized_sents):\n",
    "    vocab = {\"<s>\", \"</s>\", \"<UNK>\"}\n",
    "    all_words = Counter()\n",
    "    for sent in tokenized_sents:\n",
    "        all_words += Counter(sent)\n",
    "    return vocab.union(set([w for w,c in all_words.items() if c > 1]))\n",
    "\n",
    "vocab = get_vocab(processed_sents)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c3) Process unknown words (8 points).\n",
    "\n",
    "Write a function `process_unk(tokenized_sents, vocab)` that takes in (1) tokenized sentences returned by `process_text`, and (2) a vocabulary (set of words) returned by `get_vocab`. The function returns a list of tokenized sentences that is the same as `tokenized_sents` except that all words appearing only once will be replaced by the symbol `<UNK>`.\n",
    "\n",
    "```\n",
    ">>> print(processed_sents[3])\n",
    "['of', 'south', 'park', ',', 'in', 'the', 'county', 'of', 'gloucester', ',', 'by', 'which', 'lady', '(', 'who', 'died', '1800', ')', 'he', 'has', 'issue', 'elizabeth', ',', 'born', 'june', '1', ',', '1785', ';', 'anne', ',', 'born', 'august', '9', ',', '1787', ';', 'a', 'still-born', 'son', ',', 'november', '5', ',', '1789', ';', 'mary', ',', 'born', 'november', '20', ',', '1791', '.', \"''\"]\n",
    ">>> processed_unk = process_unk(processed_sents, vocab)\n",
    ">>> print(processed_unk[3])\n",
    "['of', '<UNK>', '<UNK>', ',', 'in', 'the', 'county', 'of', '<UNK>', ',', 'by', 'which', 'lady', '(', 'who', 'died', '<UNK>', ')', 'he', 'has', '<UNK>', 'elizabeth', ',', 'born', '<UNK>', '1', ',', '<UNK>', ';', 'anne', ',', 'born', '<UNK>', '<UNK>', ',', '<UNK>', ';', 'a', '<UNK>', 'son', ',', 'november', '<UNK>', ',', '<UNK>', ';', 'mary', ',', 'born', 'november', '<UNK>', ',', '<UNK>', '.', \"''\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'south', 'park', ',', 'in', 'the', 'county', 'of', 'gloucester', ',', 'by', 'which', 'lady', '(', 'who', 'died', '1800', ')', 'he', 'has', 'issue', 'elizabeth', ',', 'born', 'june', '1', ',', '1785', ';', 'anne', ',', 'born', 'august', '9', ',', '1787', ';', 'a', 'still-born', 'son', ',', 'november', '5', ',', '1789', ';', 'mary', ',', 'born', 'november', '20', ',', '1791', '.', \"''\"]\n",
      "['of', '<UNK>', '<UNK>', ',', 'in', 'the', 'county', 'of', '<UNK>', ',', 'by', 'which', 'lady', '(', 'who', 'died', '<UNK>', ')', 'he', 'has', '<UNK>', 'elizabeth', ',', 'born', '<UNK>', '1', ',', '<UNK>', ';', 'anne', ',', 'born', '<UNK>', '<UNK>', ',', '<UNK>', ';', 'a', '<UNK>', 'son', ',', 'november', '<UNK>', ',', '<UNK>', ';', 'mary', ',', 'born', 'november', '<UNK>', ',', '<UNK>', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "def process_unk(tokenized_sents, vocab):\n",
    "    ret = []\n",
    "    for sent in tokenized_sents:\n",
    "        ret.append([w if w in vocab else '<UNK>' for w in sent])\n",
    "    return ret\n",
    "\n",
    "print(processed_sents[3])\n",
    "processed_unk = process_unk(processed_sents, vocab)\n",
    "# print(processed)\n",
    "print(processed_unk[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c4) N-gram frequencies (10 points).\n",
    "\n",
    "We now need to get the frequency counts -- which will allow us to calculate the conditional frequency distribution for our language model. Write a function `get_freq_dist(tokenized_sents, n)` that takes in (1) a list of tokenized sentences (such as one returned by `process_unk`), and (2) the number `n` that denotes the order of the n-gram (`n=2` means bigram, `n=3` means trigram, etc). The function will return a dictionary `freqdict` (`freqdict` can be a `Counter`) such that `freqdict[context][word]` records the number of times `word` follows `context`. You can think of `context` as the first element of a tuple in a list returned by `make_ngram_tuples`, and `word` as the second element of that tuple.\n",
    "\n",
    "```\n",
    ">>> freqdict = get_freq_dict(processed_unk, 2)\n",
    ">>> print(freqdict[('of',)])\n",
    "Counter({'<UNK>': 25, 'the': 17, 'his': 6, 'her': 6, 'kellynch': 4, 'a': 4, 'being': 4, 'their': 3, 'it': 3, 'charles': 2, 'somerset': 2, 'any': 2, 'very': 2, 'mind': 2, 'life': 2, 'all': 2, 'mr': 2, 'them': 2, 'himself': 1, 'mary': 1, 'high': 1, 'baronet': 1, 'sir': 1, 'character': 1, 'real': 1, 'ever': 1, 'respectability': 1, '.': 1, 'youth': 1, 'and': 1, 'elliot': 1, 'inferior': 1, 'again': 1, 'economy': 1, 'which': 1, ';': 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'<UNK>': 25, 'the': 17, 'his': 6, 'her': 6, 'kellynch': 4, 'a': 4, 'being': 4, 'their': 3, 'it': 3, 'charles': 2, 'somerset': 2, 'any': 2, 'very': 2, 'mind': 2, 'life': 2, 'all': 2, 'mr': 2, 'them': 2, 'himself': 1, 'mary': 1, 'high': 1, 'baronet': 1, 'sir': 1, 'character': 1, 'real': 1, 'ever': 1, 'respectability': 1, '.': 1, 'youth': 1, 'and': 1, 'elliot': 1, 'inferior': 1, 'again': 1, 'economy': 1, 'which': 1, ';': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_freq_dict(tokenized_sents, n):\n",
    "    freqdict = defaultdict(Counter)\n",
    "    for sent in tokenized_sents:\n",
    "        ngrams = make_ngram_tuples(sent, n)\n",
    "        for context, word in ngrams:\n",
    "            freqdict[context][word] += 1\n",
    "    return freqdict\n",
    "\n",
    "freqdict = get_freq_dict(processed_unk, 2)\n",
    "print(freqdict[('of',)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c5) The langauge model\n",
    "\n",
    "We are now ready to put everything together and make our langauge model! Below we have written the function `build_ngram_model(textfile, n)` that takes in a text file, the value `n` that signals the order of our n-gram, and returns a language model in the form of a `namedtuple`. All we need to do is to call the various functions you've implemented so far in (c)! (**No implementation required here**).\n",
    "\n",
    "A `namedtuple` is a versatile data structure that allows one to associate multiple data fields with one variable. Below, we created a `namedtuple` called `LanguageModel`; this `LanguageModel` consists of the following information: the n-gram order `n`, the frequency distribution dictionary `fd`, and the vocabulary (as a `set` of words) `vocab`. Now, after we make a `LanguageModel`, we will be able to access these fields using the `dot` operator, for example, `toy_lm.vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT MODIFY THIS CELL\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "def build_ngram_model(textfile, n):\n",
    "    LanguageModel = namedtuple('LanguageModel', ['n', 'fd', 'vocab'])\n",
    "    psents = process_text(textfile)\n",
    "    vocab = get_vocab(psents)\n",
    "    psentsunk = process_unk(psents, vocab)\n",
    "    fd = get_freq_dict(psentsunk, n)\n",
    "    return LanguageModel(n, fd, vocab)\n",
    "\n",
    "toy_lm = build_ngram_model(\"data/austen_short.txt\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c6) Log conditional probabilities. (10 points)\n",
    "\n",
    "The heart of the language model above is just the frequency dictionary. To make our model functional, we will need to use the frequency dictionary to calculate (log) conditional probabilities. Write a function `log_prob(lm, context, word)` that takes in a language model `lm`, `context` in the form of a tuple, and a `word`. The function returns the *add-1 (Laplacian) smoothed* conditional probability values `log P(word|context)`.\n",
    "\n",
    "We would like to calculate log probabilities, instead of raw probability values, because ultipmately we will use the language model to calculate the likelihood of a text. Multiplying many very small raw probability values may result in underflow issues (and inaccuracies) in any programming language.\n",
    "\n",
    "You will need to get the size of the vocabulary when writing this function. Keep in mind that the `<UNK>`, `<s>`, and `</s>` symbols should all be a part of your vocabulary.\n",
    "\n",
    "*Please use log with base 2 for this problem!*\n",
    "\n",
    "```\n",
    ">>> log_prob(toy_lm, ('.',), '</s>')\n",
    "-2.451695969857692\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.451695969857692\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def log_prob(lm, context, word):\n",
    "    # Use log with base 2!\n",
    "    key_total = sum(lm.fd[context].values())\n",
    "    vocab_size = len(lm.vocab)\n",
    "    return math.log((lm.fd[context][word]+1)/(key_total+vocab_size), 2)\n",
    "\n",
    "print(log_prob(toy_lm, ('.',), '</s>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c7) Perplexity (16 points)\n",
    "\n",
    "Our final step to make our langauge model functional would be to calculate perplexity of a document (e.g., a new text file). Implement the function `get_ppl(lm, newfile)` that returns the perplexity of `newfile` given language model `lm` that you built using `build_ngram_model`.\n",
    "\n",
    "Be mindful to check whether a word appears in the language model's vocabulary; if not, replace with `<UNK>`.\n",
    "Note: you should count the `<s>` and `</s>` when counting the total words in each sentence.\n",
    "\n",
    "```\n",
    ">>> get_ppl(toy_lm, \"data/test_short.txt\")\n",
    ">>> 51.98841239479734\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6845.8419512679175\n",
      "1201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.98841239479734"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_ppl(lm, testfile):\n",
    "    total_log_prob = 0\n",
    "    total_token_count = 0\n",
    "    fullstring = open(testfile).read()\n",
    "    sents = sent_tokenize(fullstring)\n",
    "    for i,sent in enumerate(sents):\n",
    "#         print(sent)\n",
    "        tokens = sent_transform(sent)\n",
    "        tokens = [t if t in lm.vocab else \"<UNK>\" for t in tokens]\n",
    "        total_token_count += len(tokens)+2\n",
    "        ngram = make_ngram_tuples(tokens, lm.n)\n",
    "        total_log_prob += sum([log_prob(lm,a,b) for (a,b) in ngram])\n",
    "    print (total_log_prob)\n",
    "    print (total_token_count)\n",
    "    entropy = -total_log_prob/total_token_count\n",
    "    return 2**entropy\n",
    "\n",
    "get_ppl(toy_lm, \"data/test_short.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Authorship attribution (10 points)\n",
    "\n",
    "If we have built language models of multiple authors, they can be used to check the author of an unknown piece of writing. Concretely, for the unknown text, we can run the file through each known author's language model, and use perplexity to predict which author the unknown text is most likely to belong to.\n",
    "\n",
    "In this problem, build two **bigram** models:\n",
    "- a  Shakespeare language model\n",
    "- a  Jane Austen language model\n",
    "\n",
    "Make sure to train these models from the full text. Once you have trained both models, use the `get_ppl` function from each language model on the file `test.txt`. \n",
    "\n",
    "Who wrote the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shakespeare_model = build_ngram_model(\"data/shakespeare.txt\", 2)\n",
    "austen_model = build_ngram_model(\"data/austen.txt\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for Shakespeare: 503.9997813172643\n",
      "Test perplexity for Austen: 380.1761809626083\n",
      "The more likely author is austen\n"
     ]
    }
   ],
   "source": [
    "shakes_ppl = get_ppl(shakespeare_model, \"data/test.txt\")\n",
    "austen_ppl = get_ppl(austen_model, \"data/test.txt\")\n",
    "\n",
    "if shakes_ppl < austen_ppl:\n",
    "    more_likely_author = 'shakespeare'\n",
    "else: \n",
    "    more_likely_author = 'austen'\n",
    "\n",
    "print(\"Test perplexity for Shakespeare: \" + str(shakes_ppl))\n",
    "print(\"Test perplexity for Austen: \" + str(austen_ppl))\n",
    "print(\"The more likely author is \" + str(more_likely_author))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## (e) Bonus: Random text generator (5 points)\n",
    "\n",
    "Now, we are ready to generate some Shakespeare text! Starting with the sentence start symbol `<s>`, at each step, use the previously generated word as context, and sample one of the top 5 words that follow this word. We stop whenever we hit `</s>`, or when we have generated a 20-word sentence, whichever is earlier.\n",
    "\n",
    "Implement a function `text_generator(bigramlm, k)` that takes a bigram langauge model---trained on our Shakespeare text---as input, and generates `k` random sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the queen and his majesty , i 'll be not so ? ' world , i have been as the \n",
      "o my life and the world ; but that , that the world ; for i have i ’ t \n",
      "[ to his head ; i will not to the king , and i am i 'll be , i \n",
      "the king , that the king . \n",
      "[ to my life is the king edward . ' th ’ s house of my life , and i \n",
      "what is this ? ” quoth i ’ ll make you are in my father ; but the king henry \n",
      "i 'll make the world . ] scene i. britain be so much . ' th ’ s no , \n",
      "[ _exeunt rosencrantz , my father ; and i 'll have been as the queen , and his own person \n",
      "[ _exeunt rosencrantz . ] \n",
      "the king ’ s palace enter king ’ t , my heart , my good , i will you , \n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def text_generator(bigramlm, n):\n",
    "    cfd = bigramlm.fd ## Need to create conditional probability distribution\n",
    "    for _ in range(n):\n",
    "        context = (\"<s>\",)\n",
    "        sentence = \"\"\n",
    "        \n",
    "        for _ in range(20):\n",
    "            \n",
    "            # We don't want UNK for generation, so remove it! \n",
    "            if \"<UNK>\" in cfd[context]:\n",
    "                del cfd[context][\"<UNK>\"]\n",
    "\n",
    "            # sort the potential words, and just take the first 5\n",
    "            next_words = sorted(cfd[context].items(), key=lambda item: item[1], reverse=True)\n",
    "            if len(next_words) > 5:\n",
    "                next_words = next_words[:5]\n",
    "                \n",
    "            # pick a random top word \n",
    "            next_word = next_words[randint(0,len(next_words)-1)][0]\n",
    "\n",
    "            # end if end of sentence\n",
    "            if next_word == \"</s>\":\n",
    "                break\n",
    "                \n",
    "            # set the next word to be the context\n",
    "            context = (next_word,)\n",
    "            \n",
    "            # add the next word to the sentence\n",
    "            sentence += next_word + \" \"\n",
    "        print(sentence)\n",
    "            \n",
    "text_generator(shakespeare_model, 10)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "9ee92ef7ac9c4e76a3443c477335478f09402823207bba7eb8b055205da5967f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
