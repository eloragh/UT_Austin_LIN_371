{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-737ecffa804edefe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# LIN 371 Machine Learning for Text Analysis\n",
    "\n",
    "# Homework 2 - due Friday Feb 16 2024 at 11:59pm\n",
    "\n",
    "For this homework you will hand in (upload) to canvas your code named ``hw2_YourEID.ipynb``.\n",
    "\n",
    "__Before submitting__, please reset your kernel and rerun everything from the beginning (`Kernel` >> `Restart and Run All`) and ensure your code outputs the correct answer.\n",
    "\n",
    "A perfect solution for this homework is worth **100** points, and there is a bonus problem at the end that is worth **5 points**. For non-programming tasks, you must show work to get partial credit. For programming tasks, make sure that your code can run using Python 3.5+. If you cannot complete a problem, include as much pseudocode as possible for partial credit. However, make sure it does not have any output errors. **If there are any output errors, half of the points for that problem will be automatically deducted.**\n",
    "\n",
    "Collaboration: you are free to discuss the homework assignments with other students and work towards solutions together.  However, all of the code you write must be your own! There is a channel on Slack where you can look for a study group.\n",
    "\n",
    "Review extension and academic dishonesty policy here: https://jessyli.com/courses/lin371\n",
    "\n",
    "For typing up your answers to problems 1, 2 and 3, information can be found about Markdown cells for Jupyter Notebooks here: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "\n",
    "\n",
    "### Please list any collaborators here:\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e27014b61dce338",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 1: Naive Bayes\n",
    "\n",
    "We'll do a bit of manual parameter estimation here.\n",
    "\n",
    "## **(a)** (8 points) \n",
    "Calculate the sufficient parameters for Naive Bayes using the data in the figure below, that\n",
    "is, the prior class probabilities and the conditional probabilities for all of the symbols.\n",
    "\n",
    "doc | X | Y\n",
    "-- | --| --\n",
    "d1 | a b b b c b | +\n",
    "d2 | c a c c c b | -\n",
    "d3 | c c c b | -\n",
    "d4 | b a b b b | +\n",
    "d5 | b c a b b | ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize the training data and classes\n",
    "docs_train = ['a b b b c b',\n",
    "              'c a c c c b',\n",
    "              'c c c b',\n",
    "              'b a b b b']\n",
    "Y_train = [1, 0, 0, 1]\n",
    "\n",
    "# tokenize train and test data\n",
    "docs_train_tokenized = [doc.split() for doc in docs_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2}\n"
     ]
    }
   ],
   "source": [
    "# create IDs for each character that will serve as columns in the matrix\n",
    "char_to_col_id = {}\n",
    "\n",
    "for doc in docs_train_tokenized:\n",
    "    for char in doc:\n",
    "        if char not in char_to_col_id:\n",
    "            char_to_col_id[char] = len(char_to_col_id)\n",
    "\n",
    "print(char_to_col_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 4. 1.]\n",
      " [1. 1. 4.]\n",
      " [0. 1. 3.]\n",
      " [1. 4. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# initialize matrix\n",
    "X_train = np.zeros((len(docs_train), len(char_to_col_id)))\n",
    "\n",
    "# column represents the word\n",
    "# row represents the doc\n",
    "for i, doc in enumerate(docs_train_tokenized):\n",
    "    for word in doc:\n",
    "        rowid = i\n",
    "        colid = char_to_col_id[word]\n",
    "        X_train[rowid][colid] += 1\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 prior = 0.5\n",
      "conditional probability of a in class 0 is 0.15384615384615385\n",
      "conditional probability of b in class 0 is 0.23076923076923078\n",
      "conditional probability of c in class 0 is 0.6153846153846154\n",
      "\n",
      "class 1 prior = 0.5\n",
      "conditional probability of a in class 1 is 0.21428571428571427\n",
      "conditional probability of b in class 1 is 0.6428571428571429\n",
      "conditional probability of c in class 1 is 0.14285714285714285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "prior = [0,0]\n",
    "condprob = defaultdict(dict)\n",
    "\n",
    "# P(c|x) = P(x|c)P(x)\n",
    "    # Posterior = P(x|c) = number of times x appears in class c\n",
    "    # Prior = P(x) = number of times class c occured out of all instances\n",
    "\n",
    "for c in [0,1]:\n",
    "    prior[c] = Y_train.count(c)/len(Y_train)\n",
    "\n",
    "    # summing all items for class c\n",
    "    # this will serve as the denominator for our posterior\n",
    "    countallc = sum([sum(X_train[i]) for i, y in enumerate(Y_train) if y == c])\n",
    "\n",
    "    print('class', c, 'prior =', prior[c])\n",
    "\n",
    "    for token, tid in char_to_col_id.items():\n",
    "        # sums number of time each token occurs for each class\n",
    "        # wills serve as the numerator for our posterior\n",
    "        count_tc = sum([X_train[i][tid] for i, y in enumerate(Y_train) if y == c])\n",
    "\n",
    "        # populate the conditional probabilities\n",
    "        # laplace smoothing, +1 to numerator, +# of tokens to denominator\n",
    "        condprob[tid][c] = (count_tc + 1)/(countallc + len(char_to_col_id))\n",
    "        \n",
    "        print('conditional probability of', token, 'in class', c, 'is', condprob[tid][c])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **(b)** (7 points) \n",
    "Using these, calculate the most likely class (pos or neg) for the unlabeled example (d5, labeled ???)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "docs_test = ['b c a b b']\n",
    "docs_test_tokenized = [doc.split() for doc in docs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.zeros((len(docs_test),len(char_to_col_id)))\n",
    "\n",
    "for i, doc in enumerate(docs_test_tokenized):\n",
    "    for word in doc:\n",
    "        rowid = i\n",
    "        if word in char_to_col_id:\n",
    "            colid = char_to_col_id[word]\n",
    "            X_test[rowid][colid] += 1\n",
    "\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 0.0005817508005806736\n",
      "class 1 0.0040663860296305115\n",
      "\n",
      "The probability of d5 being in class 0 is 0.1251578475055869\n",
      "The probability of d5 being in class 1 is 0.8748421524944131\n"
     ]
    }
   ],
   "source": [
    "testprob = [0,0]\n",
    "\n",
    "for c in [0, 1]:\n",
    "    testprob[c] = prior[c]\n",
    "    for i, x in enumerate(X_test[0]):\n",
    "        testprob[c] = testprob[c] * (condprob[i][c] ** x)\n",
    "    \n",
    "    print('class', c, testprob[c])\n",
    "print()\n",
    "\n",
    "for c in [0,1]:\n",
    "    print('The probability of d5 being in class', c, 'is', testprob[c]/sum(testprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Language Modeling\n",
    "\n",
    "In this problem, we will implement our own Shakespeare and Austen language models!\n",
    "\n",
    "### Data\n",
    "\n",
    "We will be working with a few different files under the `data` subdirectory. \n",
    "- `shakespeare.txt`\n",
    "- `shakespeare_short.txt`\n",
    "- `austen.txt`\n",
    "- `austen_short.txt`\n",
    "- `test_short.txt`\n",
    "\n",
    "<mark>You should use the \"short\" text files to do testing and debugging</mark> as the other files will take a little while (but should be under 10 mins for a 8G RAM laptop) to run! \n",
    "\n",
    "### Note re NLTK\n",
    "\n",
    "The goal of this problem is to think about how to implement language models from first principles. Thus, **the *ONLY* allowable functions from NTLK in this problem are `sent_tokenize` and `word_tokenize`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Clean text. (5 points) \n",
    "\n",
    "Create a function `sent_transform(sent_string)`, such that when given a sentence as a string, it would return a `list` of words, lower-cased. Use NLTK's `word_tokenize` function to tokenize the sentence. For now, you can use any random string to test this function.\n",
    "\n",
    "```\n",
    ">>> sent_transform(\"ROSALIND. Why, whither shall we go?\")\n",
    "['rosalind', '.', 'why', ',', 'whither', 'shall', 'we', 'go', '?']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rosalind', '.', 'why', ',', 'whither', 'shall', 'we', 'go', '?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sent_transform(sent_string):\n",
    "    ## YOUR CODE HERE\n",
    "    from nltk import word_tokenize\n",
    "    \n",
    "    sent_tokenized = word_tokenize(sent_string.lower())\n",
    "    return sent_tokenized\n",
    "\n",
    "sent_transform(\"ROSALIND. Why, whither shall we go?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Find n-grams. (10 points) \n",
    "\n",
    "Create a function `make_ngram_tuples(words, n)` that returns a sequence of all n-grams seen in the input, in order.  The function returns a sequence of tuples where each tuple is of the form `(context, word)`.  The context is a tuple of the preceding n−1 words for each word; if the number of preceding words is smaller than n−1, place a `<s>` symbol in place of each missing context.  Close each sentence with an additional token `</s>`.  You can assumen n>1, that is, we are interested in enumerating bigrams, trigrams etc, and not unigrams.\n",
    "\n",
    "For now, you can use any random string to test this function.\n",
    "\n",
    "```\n",
    ">>> samples = [’she’, ’eats’, ’happily’ ’.’]\n",
    ">>> make_ngram_tuples(samples, 2)\n",
    "[((’<s>’,), ’she’), ((’she’,), ’eats’), ((’eats’,), ’happily’), ((’happily’,), ’.’), ((’.’,), ’</s>’)]\n",
    ">>> make_ngram_tuples(samples, 3)\n",
    "\n",
    "[((’<s>’, ’<s>’), ’she’), ((’<s>’, ’she’), ’eats’), ((’she’, ’eats’), ’happily’),((’eats’, ’happily’), ’.’), ((’happily’, ’.’), ’</s>’)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('<s>',), 'she'),\n",
       " (('she',), 'eats'),\n",
       " (('eats',), 'happily'),\n",
       " (('happily',), '.'),\n",
       " (('.',), '</s>')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_ngram_tuples(words, n):\n",
    "    ## YOUR CODE HERE\n",
    "    words.insert(0, \"<s>\")\n",
    "    words.insert(len(words), \"</s>\")\n",
    "\n",
    "    ngrams = []\n",
    "\n",
    "    # enumerate over the words\n",
    "    for idx, word in enumerate(words):\n",
    "        # initialize a list to hold the prior\n",
    "        prior = []\n",
    "        # loop up to n + 1 to include n\n",
    "        # start at 1 because 0 will always be a <s>\n",
    "        for i in range(1, n + 1):\n",
    "            # don't want to generate ngrams for <s>\n",
    "            if idx > 0:\n",
    "                # until we hit n, we want to add to the prior\n",
    "                if i < n:\n",
    "                    # if we reach 0 before i == n, just keep adding the 0th element\n",
    "                    prior.insert(-1, words[idx - i if idx - i > 0 else 0])\n",
    "                # put the ngram together and append the list\n",
    "                elif i == n:\n",
    "                    n_gram = (tuple(prior), word)\n",
    "                    ngrams.append(n_gram)\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "make_ngram_tuples(['she', 'eats', 'happily', '.'], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)  Building an n-gram language model\n",
    "\n",
    "We are now ready to estimate a bigram language model from the training data.  We will use **Laplace smoothing (add-1)**.\n",
    "\n",
    "### (c1) Process the training file (8 points)\n",
    "\n",
    "We will first need to transform a file of plain text into a list of tokenized \"sentences\", where each \"sentence\" is a list of *lower-cased* words. Implement a function `process_text(textfile)` to do so. Assume that the path of `textfile` is of a form like `data/austen_short.txt`, i.e., relative paths. Make sure that you call `sent_transform` in this function.\n",
    "\n",
    "```\n",
    ">>> processed_sents = process_text(\"data/austen_short.txt\")\n",
    ">>> print(processed_sents[10])\n",
    "['he', 'considered', 'the', 'blessing', 'of', 'beauty', 'as', 'inferior', 'only', 'to', 'the', 'blessing', 'of', 'a', 'baronetcy', ';', 'and', 'the', 'sir', 'walter', 'elliot', ',', 'who', 'united', 'these', 'gifts', ',', 'was', 'the', 'constant', 'object', 'of', 'his', 'warmest', 'respect', 'and', 'devotion', '.']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'considered', 'the', 'blessing', 'of', 'beauty', 'as', 'inferior', 'only', 'to', 'the', 'blessing', 'of', 'a', 'baronetcy', ';', 'and', 'the', 'sir', 'walter', 'elliot', ',', 'who', 'united', 'these', 'gifts', ',', 'was', 'the', 'constant', 'object', 'of', 'his', 'warmest', 'respect', 'and', 'devotion', '.']\n"
     ]
    }
   ],
   "source": [
    "def process_text(textfile):\n",
    "\n",
    "    from nltk import sent_tokenize\n",
    "    # read the file\n",
    "    f = open(textfile).read()\n",
    "    # tokenize the sentences\n",
    "    processed_sents = sent_tokenize(f)\n",
    "    # process each tokenized sentence using sent_transform\n",
    "    processed_sents = [sent_transform(sent) for sent in processed_sents]\n",
    "\n",
    "    return processed_sents\n",
    "\n",
    "processed_sents = process_text(\"data/austen_short.txt\")\n",
    "print(processed_sents[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c2) Vocabulary (8 points).\n",
    "\n",
    "The language model ought to be able to handle words not seen in the training data.  Such words will most certainly appear if the LM is used in some application to estimate the likelihood of new data.   There  are  many  ways  to  incorporate  unknown  vocabulary  in  a  language  model.   In  this problem, we will take all words that appear only once and replace them with the symbol `<UNK>`. Thus, at test time, if we encounter an out-of-vocabulary word, we can resort to replacing the word with `<UNK>` which has well-formed probabilities.\n",
    "\n",
    "First, implement a function `get_vocab(tokenized_sents)` where the parameter `tokenized_sents` is a list of tokenized \"sentences\" (where each \"sentence\" is a list of lower-cased words) returned by the function `process_text`. The function `get_vocab` will return a `set` of all unique words in `tokenized_sents` that appeared more than once.\n",
    "Note: do not forget to add three extra tokens to the vocabulary: `<s>`, `</s>`, and `<UNK>`.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    ">>> vocab = get_vocab(processed_sents)\n",
    ">>> print(len(vocab))\n",
    "302\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(tokenized_sents):\n",
    "\n",
    "    # initialize a dictionary with two items\n",
    "    word_dict = {'<s>': 0, '</s>': 0}\n",
    "\n",
    "    # loop through the sentences\n",
    "    for sent in tokenized_sents:\n",
    "        # each sentence adds one to our start and end paddings\n",
    "        word_dict['<s>'] += 1\n",
    "        word_dict['</s>'] += 1\n",
    "        # count the number of occurences of each word\n",
    "        for word in sent:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "    \n",
    "    # add UNK to the dict with a value of the number of words that only appear once\n",
    "    word_dict['<UNK>'] = len([key for key in word_dict.keys() if word_dict[key] == 1])\n",
    "\n",
    "    # only return the set of keys that have a value greater than one\n",
    "    return set((key for key in word_dict.keys() if word_dict[key] > 1))\n",
    "\n",
    "vocab = get_vocab(processed_sents)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c3) Process unknown words (8 points).\n",
    "\n",
    "Write a function `process_unk(tokenized_sents, vocab)` that takes in (1) tokenized sentences returned by `process_text`, and (2) a vocabulary (set of words) returned by `get_vocab`. The function returns a list of tokenized sentences that is the same as `tokenized_sents` except that all words appearing only once will be replaced by the symbol `<UNK>`.\n",
    "\n",
    "```\n",
    ">>> print(processed_sents[3])\n",
    "['of', 'south', 'park', ',', 'in', 'the', 'county', 'of', 'gloucester', ',', 'by', 'which', 'lady', '(', 'who', 'died', '1800', ')', 'he', 'has', 'issue', 'elizabeth', ',', 'born', 'june', '1', ',', '1785', ';', 'anne', ',', 'born', 'august', '9', ',', '1787', ';', 'a', 'still-born', 'son', ',', 'november', '5', ',', '1789', ';', 'mary', ',', 'born', 'november', '20', ',', '1791', '.', \"''\"]\n",
    ">>> processed_unk = process_unk(processed_sents, vocab)\n",
    ">>> print(processed_unk[3])\n",
    "['of', '<UNK>', '<UNK>', ',', 'in', 'the', 'county', 'of', '<UNK>', ',', 'by', 'which', 'lady', '(', 'who', 'died', '<UNK>', ')', 'he', 'has', '<UNK>', 'elizabeth', ',', 'born', '<UNK>', '1', ',', '<UNK>', ';', 'anne', ',', 'born', '<UNK>', '<UNK>', ',', '<UNK>', ';', 'a', '<UNK>', 'son', ',', 'november', '<UNK>', ',', '<UNK>', ';', 'mary', ',', 'born', 'november', '<UNK>', ',', '<UNK>', '.', \"''\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'south', 'park', ',', 'in', 'the', 'county', 'of', 'gloucester', ',', 'by', 'which', 'lady', '(', 'who', 'died', '1800', ')', 'he', 'has', 'issue', 'elizabeth', ',', 'born', 'june', '1', ',', '1785', ';', 'anne', ',', 'born', 'august', '9', ',', '1787', ';', 'a', 'still-born', 'son', ',', 'november', '5', ',', '1789', ';', 'mary', ',', 'born', 'november', '20', ',', '1791', '.', \"''\"]\n",
      "['of', '<UNK>', '<UNK>', ',', 'in', 'the', 'county', 'of', '<UNK>', ',', 'by', 'which', 'lady', '(', 'who', 'died', '<UNK>', ')', 'he', 'has', '<UNK>', 'elizabeth', ',', 'born', '<UNK>', '1', ',', '<UNK>', ';', 'anne', ',', 'born', '<UNK>', '<UNK>', ',', '<UNK>', ';', 'a', '<UNK>', 'son', ',', 'november', '<UNK>', ',', '<UNK>', ';', 'mary', ',', 'born', 'november', '<UNK>', ',', '<UNK>', '.', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "def process_unk(tokenized_sents, vocab):\n",
    "    ## YOUR CODE HERE\n",
    "    processed_unk = []\n",
    "\n",
    "    for sent in tokenized_sents:\n",
    "        processed_unk_sent = []\n",
    "        for word in sent:\n",
    "            if word in vocab:\n",
    "                processed_unk_sent.append(word)\n",
    "            else:\n",
    "                processed_unk_sent.append('<UNK>')\n",
    "        processed_unk.append(processed_unk_sent)\n",
    "\n",
    "    return processed_unk\n",
    "\n",
    "print(processed_sents[3])\n",
    "processed_unk = process_unk(processed_sents, vocab)\n",
    "print(processed_unk[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c4) N-gram frequencies (10 points).\n",
    "\n",
    "We now need to get the frequency counts -- which will allow us to calculate the conditional frequency distribution for our language model. Write a function `get_freq_dist(tokenized_sents, n)` that takes in (1) a list of tokenized sentences (such as one returned by `process_unk`), and (2) the number `n` that denotes the order of the n-gram (`n=2` means bigram, `n=3` means trigram, etc). The function will return a dictionary `freqdict` (`freqdict` can be a `Counter`) such that `freqdict[context][word]` records the number of times `word` follows `context`. You can think of `context` as the first element of a tuple in a list returned by `make_ngram_tuples`, and `word` as the second element of that tuple.\n",
    "\n",
    "```\n",
    ">>> freqdict = get_freq_dict(processed_unk, 2)\n",
    ">>> print(freqdict[('of',)])\n",
    "Counter({'<UNK>': 25, 'the': 17, 'his': 6, 'her': 6, 'kellynch': 4, 'a': 4, 'being': 4, 'their': 3, 'it': 3, 'charles': 2, 'somerset': 2, 'any': 2, 'very': 2, 'mind': 2, 'life': 2, 'all': 2, 'mr': 2, 'them': 2, 'himself': 1, 'mary': 1, 'high': 1, 'baronet': 1, 'sir': 1, 'character': 1, 'real': 1, 'ever': 1, 'respectability': 1, '.': 1, 'youth': 1, 'and': 1, 'elliot': 1, 'inferior': 1, 'again': 1, 'economy': 1, 'which': 1, ';': 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('of',)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[0;32m      5\u001b[0m freqdict \u001b[38;5;241m=\u001b[39m get_freq_dict(processed_unk, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfreqdict\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mof\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mKeyError\u001b[0m: ('of',)"
     ]
    }
   ],
   "source": [
    "def get_freq_dict(tokenized_sents, n):\n",
    "    ## YOUR CODE HERE\n",
    "    return {}\n",
    "\n",
    "freqdict = get_freq_dict(processed_unk, 2)\n",
    "print(freqdict[('of',)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c5) The langauge model\n",
    "\n",
    "We are now ready to put everything together and make our langauge model! Below we have written the function `build_ngram_model(textfile, n)` that takes in a text file, the value `n` that signals the order of our n-gram, and returns a language model in the form of a `namedtuple`. All we need to do is to call the various functions you've implemented so far in (c)! (**No implementation required here**).\n",
    "\n",
    "A `namedtuple` is a versatile data structure that allows one to associate multiple data fields with one variable. Below, we created a `namedtuple` called `LanguageModel`; this `LanguageModel` consists of the following information: the n-gram order `n`, the frequency distribution dictionary `fd`, and the vocabulary (as a `set` of words) `vocab`. Now, after we make a `LanguageModel`, we will be able to access these fields using the `dot` operator, for example, `toy_lm.vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_unk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     fd \u001b[38;5;241m=\u001b[39m get_freq_dict(psentsunk, n)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LanguageModel(n, fd, vocab)\n\u001b[1;32m---> 13\u001b[0m toy_lm \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_ngram_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/austen_short.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m, in \u001b[0;36mbuild_ngram_model\u001b[1;34m(textfile, n)\u001b[0m\n\u001b[0;32m      7\u001b[0m psents \u001b[38;5;241m=\u001b[39m process_text(textfile)\n\u001b[0;32m      8\u001b[0m vocab \u001b[38;5;241m=\u001b[39m get_vocab(psents)\n\u001b[1;32m----> 9\u001b[0m psentsunk \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_unk\u001b[49m(psents, vocab)\n\u001b[0;32m     10\u001b[0m fd \u001b[38;5;241m=\u001b[39m get_freq_dict(psentsunk, n)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LanguageModel(n, fd, vocab)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'process_unk' is not defined"
     ]
    }
   ],
   "source": [
    "## DO NOT MODIFY THIS CELL, but you need to run it.\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "def build_ngram_model(textfile, n):\n",
    "    LanguageModel = namedtuple('LanguageModel', ['n', 'fd', 'vocab'])\n",
    "    psents = process_text(textfile)\n",
    "    vocab = get_vocab(psents)\n",
    "    psentsunk = process_unk(psents, vocab)\n",
    "    fd = get_freq_dict(psentsunk, n)\n",
    "    return LanguageModel(n, fd, vocab)\n",
    "\n",
    "toy_lm = build_ngram_model(\"data/austen_short.txt\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c6) Log conditional probabilities. (10 points)\n",
    "\n",
    "The heart of the language model above is just the frequency dictionary. To make our model functional, we will need to use the frequency dictionary to calculate (log) conditional probabilities. Write a function `log_prob(lm, context, word)` that takes in a language model `lm`, `context` in the form of a tuple, and a `word`. The function returns the *add-1 (Laplacian) smoothed* conditional probability values `log P(word|context)`.\n",
    "\n",
    "We would like to calculate log probabilities, instead of raw probability values, because ultipmately we will use the language model to calculate the likelihood of a text. Multiplying many very small raw probability values may result in underflow issues (and inaccuracies) in any programming language.\n",
    "\n",
    "You will need to get the size of the vocabulary when writing this function. Keep in mind that the `<UNK>`, `<s>`, and `</s>` symbols should all be a part of your vocabulary.\n",
    "\n",
    "*Please use log with base 2 for this problem!*\n",
    "\n",
    "\n",
    "```\n",
    ">>> log_prob(toy_lm, ('.',), '</s>')\n",
    "-2.451695969857692\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(lm, context, word):\n",
    "    ## YOUR CODE HERE\n",
    "    # Use log with base 2!\n",
    "    return 0\n",
    "\n",
    "print(log_prob(toy_lm, ('.',), '</s>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c7) Perplexity (16 points)\n",
    "\n",
    "Our final step to make our langauge model functional would be to calculate perplexity of a document (e.g., a new text file). Implement the function `get_ppl(lm, newfile)` that returns the perplexity of `newfile` given language model `lm` that you built using `build_ngram_model`.\n",
    "\n",
    "Be mindful to check whether a word appears in the language model's vocabulary; if not, replace with `<UNK>`.\n",
    "\n",
    "```\n",
    ">>> get_ppl(toy_lm, \"data/test_short.txt\")\n",
    ">>> 51.98841239479734\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppl(lm, testfile):\n",
    "    ## YOUR CODE HERE\n",
    "    return 0\n",
    "\n",
    "get_ppl(toy_lm, \"data/test_short.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Authorship attribution (10 points)\n",
    "\n",
    "If we have built language models of multiple authors, they can be used to check the author of an unknown piece of writing. Concretely, for the unknown text, we can run the file through each known author's language model, and use perplexity to predict which author the unknown text is most likely to belong to.\n",
    "\n",
    "In this problem, build two **bigram** models:\n",
    "- a  Shakespeare language model\n",
    "- a  Jane Austen language model\n",
    "\n",
    "Make sure to train these models from the full text. Once you have trained both models, use the `get_ppl` function from each language model on the file `test.txt`. \n",
    "\n",
    "Who wrote the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## (e) Bonus: Random text generator (5 points)\n",
    "\n",
    "Now, we are ready to generate some Shakespeare text! Starting with the sentence start symbol `<s>`, at each step, use the previously generated word as context, and sample one of the top 5 words that follow this word. We stop whenever we hit `</s>`, or when we have generated a 20-word sentence, whichever is earlier.\n",
    "\n",
    "Implement a function `text_generator(bigramlm, k)` that takes a bigram langauge model---trained on our Shakespeare text---as input, and generates `k` random sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(bigramlm, k):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "9ee92ef7ac9c4e76a3443c477335478f09402823207bba7eb8b055205da5967f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
