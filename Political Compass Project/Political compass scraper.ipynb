{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit import Client\n",
    "from twikit import TwitterException \n",
    "from twikit import TooManyRequests\n",
    "from twikit.utils import Endpoint\n",
    "from translate import Translator\n",
    "from math import ceil\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this API requires authentication\n",
    "f = open('authentication.txt', 'r')\n",
    "auth = f.read()\n",
    "f.close()\n",
    "auth_token = auth.split(\"\\n\")\n",
    "\n",
    "# don't hardcode your email and password into something!!!\n",
    "# the auth is in gitignore so I won't get hacked\n",
    "USERNAME = str(auth_token[0])\n",
    "EMAIL = str(auth_token[1])\n",
    "PASSWORD = str(auth_token[2])\n",
    "\n",
    "# Initialize client\n",
    "client = Client(language='en-US', http2=True)\n",
    "\n",
    "# Login to the service with provided user credentials\n",
    "client.login(\n",
    "    auth_info_1=USERNAME ,\n",
    "    auth_info_2=EMAIL,\n",
    "    password=PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter LOVES to ban people when they log in repeatedly\n",
    "# saving the cookies makes sure I don't get banned (often)\n",
    "\n",
    "client.get_cookies()\n",
    "client.save_cookies('IGNOREcookies.json')\n",
    "with open('IGNOREcookies.json', 'r', encoding='UTF8') as f:\n",
    "    client.set_cookies(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housekeeping function\n",
    "# each different method uses a different API endpoint\n",
    "# each different API endpoint has a rate limit\n",
    "# you can hit it a certain number of times per a time period (usually 15 minutes)\n",
    "# this tells me how much time I have left if I've hit the rate limit\n",
    "\n",
    "def get_limit_reset_time(endpoint: str):\n",
    "    res = requests.get(\n",
    "        endpoint,\n",
    "        headers=client._base_headers,\n",
    "        cookies=client.get_cookies()\n",
    "    )\n",
    "    return ceil(int(res.headers['x-rate-limit-reset']) - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeout check for scraping tweet IDs\n",
    "try:\n",
    "    print(client.search_tweet(\n",
    "        f'from:JoeBiden since:2020-01-01 until:2021-03-01', 'Latest', count=40\n",
    "    ))\n",
    "except TooManyRequests:\n",
    "    reset_time = get_limit_reset_time(Endpoint.USER_TWEETS)\n",
    "    print(f'rate limit is reset after {reset_time} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeout check for processing tweets\n",
    "try:\n",
    "    print(client.get_tweet_by_id(1351951465674276869))\n",
    "except TooManyRequests:\n",
    "    reset_time = get_limit_reset_time(Endpoint.USER_TWEETS)\n",
    "    print(f'rate limit is reset after {reset_time} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another housekeeping function\n",
    "# if I'm suddenly getting 403 errors, I can use this to check if I've been banned\n",
    "# sometimes I just have to go on the browser and reauthenticate\n",
    "\n",
    "def check_user_status(user_id):\n",
    "    \"\"\"\n",
    "    True if the user is active, otherwise false (not exists or suspended).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.get_user_by_id(user_id)\n",
    "    except TwitterException as e:\n",
    "        if str(e).startswith('Invalid user id'):\n",
    "            return False\n",
    "        raise e\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "check_user_status(1547081484695216130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: the user handle, a beginning and end of a date range\n",
    "# OUTPUT: the user handle, the user_ID, and the scraped tweets\n",
    "def get_all_tweets(handle, since, until):\n",
    "\n",
    "    # load the cookies so you don't login a million times and get banned\n",
    "    client.load_cookies('IGNOREcookies.json')\n",
    "\n",
    "    # initialize the list we will store our data in\n",
    "    mass_tweets = []\n",
    "\n",
    "    since = f'{since}-01-01'\n",
    "    until = f'{until}-12-31'\n",
    "\n",
    "    # this will pull the first forty tweets\n",
    "    tweets = client.search_tweet(\n",
    "        f'from:{handle} since:{since} until:{until}', 'Top'\n",
    "    )\n",
    "    tweets1 = [tweet.id for tweet in tweets]\n",
    "    mass_tweets += tweets1\n",
    "\n",
    "    # if it returns an empty list, the user had no available tweets during the date time range\n",
    "    if len(tweets) == 0:\n",
    "        return([])\n",
    "    \n",
    "    # this will keep looking for tweets until a certain number of them has been reached\n",
    "    while len(tweets) > 0 and len(mass_tweets) < 30:\n",
    "\n",
    "        # this API provides a 'tweet' object, but we only want the id when we return\n",
    "        tweets = tweets.next()\n",
    "        tweets1 = [tweet.id for tweet in tweets]\n",
    "        mass_tweets += tweets1\n",
    "\n",
    "        # keep pulling tweets until number is hit or there are none left\n",
    "\n",
    "        # we need to make a check in case we've hit the max number of tweets we can scrape\n",
    "        # this prevents us from pinging the API for no reason\n",
    "        if len(tweets) == 0:\n",
    "            print(\"No more tweets\")\n",
    "            break\n",
    "        else:\n",
    "            print(f'Total tweets = {len(mass_tweets)}')\n",
    "            continue\n",
    "\n",
    "    return(mass_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(handle, user_id, name, tweet_ids, since):\n",
    "    # load the cookies so you don't login a million times and get banned\n",
    "    client.load_cookies('IGNOREcookies.json')\n",
    "\n",
    "    # initialize a list to store all tuples\n",
    "    tweets = []\n",
    "\n",
    "    i = 0\n",
    "    for tweet_id in tweet_ids:\n",
    "        try:\n",
    "            # using the IDs we pulled from above\n",
    "            tweet = client.get_tweet_by_id(tweet_id)\n",
    "\n",
    "            # we have international data\n",
    "            # this will translate it and identify it's translation\n",
    "            if tweet.lang != 'en':\n",
    "                translator = Translator(to_lang='en')\n",
    "                tweet_text = translator.translate(tweet.text)\n",
    "                tweets.append([int(tweet.id), int(user_id), name, handle, str(tweet_text), str(tweet.lang), 'True', 'en', str(tweet.created_at_datetime), since])\n",
    "\n",
    "            # otherwise we just move on\n",
    "            else:\n",
    "                tweets.append((int(tweet.id), int(user_id), name, handle, str(tweet.text), str(tweet.lang), 'False', 'null', str(tweet.created_at_datetime), since))\n",
    "        \n",
    "            print(f'Tweet {i} processed')\n",
    "            i += 1\n",
    "\n",
    "        # it throws an Index Error if the tweet has been deleted/ is not available\n",
    "        except IndexError:\n",
    "            print(f'Index Error: {tweet}')\n",
    "        \n",
    "        time.sleep(.5)\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('tweets.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"\"\" SELECT twitter_user_id, politician_name, twitter_handle, election_year\n",
    "            FROM coordinates\n",
    "            WHERE twitter_active_during_election = 'True'\n",
    "            \"\"\")\n",
    "active_user_list = c.fetchall()\n",
    "print(active_user_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = active_user_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tweet_list = []\n",
    "no_tweets_list = []\n",
    "\n",
    "while len(user_list) > 0:\n",
    "\n",
    "    tweet_id_list = []\n",
    "    i = 0\n",
    "\n",
    "    # this first loop pulls tweet IDs for politicians\n",
    "    while (i < 5 and len(user_list) > 0):\n",
    "        \n",
    "        # we're only ever concerned with the first value\n",
    "        # we pop this later to keep the list moving\n",
    "        lst = user_list[0]\n",
    "\n",
    "        # all of our parameters for the functions\n",
    "        user_id = lst[0]\n",
    "        name = lst[1]\n",
    "        handle = lst[2]\n",
    "        since = lst[3]\n",
    "        until = lst[3]+1\n",
    "\n",
    "        # pull tweet IDs that we will user in second while loop\n",
    "        tweet_ids = get_all_tweets(handle, since=since, until=until)\n",
    "        print(f'{len(tweet_ids)} tweets collected for {name} for {since} election')\n",
    "\n",
    "        # pop user so we're not in an infinite loop\n",
    "        user_list.pop(0)\n",
    "\n",
    "        # we only add to the counter if that person actually had tweets to process\n",
    "        if len(tweet_ids) > 0:\n",
    "            i+=1\n",
    "            tweet_id_list.append([handle, user_id, name, tweet_ids, since])\n",
    "        \n",
    "        # I want to keep track of which politicians didn't tweet during their election year\n",
    "        else:\n",
    "            no_tweets_list.append([user_id, name, handle, since])\n",
    "        \n",
    "    tweet_id_list_copy = tweet_id_list.copy()\n",
    "    tweet_list = []\n",
    "\n",
    "    # this second loop processes and cleans the tweets\n",
    "    # it formats each tweet into a list that can be executed into the SQL database\n",
    "    while len(tweet_id_list_copy) > 0:\n",
    "        \n",
    "        # same thing here, we're only ever concerned with index 0\n",
    "        # these are lists of lists\n",
    "        index = tweet_id_list_copy[0]\n",
    "\n",
    "        # all the parameters we need\n",
    "        handle = index[0]\n",
    "        user_id = index[1]\n",
    "        name = index[2]\n",
    "        year = index[4]\n",
    "        \n",
    "        # we only want to do up to 25 tweets per politician\n",
    "        # this means we can process 5 politicians per rate timeout\n",
    "        # 150 tweets per 15 minutes\n",
    "        # not great but it's free\n",
    "\n",
    "        if len(index[3]) > 25:\n",
    "\n",
    "            # random sample in an attempt to stay unbiased\n",
    "            tweet_ids = random.sample(index[3], 25)\n",
    "        else:\n",
    "            tweet_ids = index[3]\n",
    "        \n",
    "        # process the tweets and add them to our holding list from above\n",
    "        tweets = process_tweets(handle, user_id, name, tweet_ids, year)\n",
    "        tweet_list.append(tweets)\n",
    "        \n",
    "        # pop the first index to keep the loop going\n",
    "        tweet_id_list_copy.pop(0)\n",
    "    \n",
    "    # once we've run both loops, we add to the final list\n",
    "    # this way we don't lose the data once it restarts\n",
    "    final_tweet_list += tweet_list\n",
    "\n",
    "    # rate limit protection\n",
    "    # we can only process 150 tweets per 15 minutes\n",
    "    # getting errors from rate limits throws off the whole thing\n",
    "    # this prevents that from happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in final_tweet_list[0]:\n",
    "    print(type(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('tweets.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"\"\"CREATE TABLE tweets (\n",
    "          tweet_id INTEGER,\n",
    "          user_id INTEGER,\n",
    "          user_name STRING,\n",
    "          user_handle STRING,\n",
    "          tweet_text STRING,\n",
    "          tweet_original_lang STRING,\n",
    "          tweet_translated STRING,\n",
    "          tweet_translated_lang STRING,\n",
    "          created_date STRING,\n",
    "          election_year INT\n",
    "          )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute('DROP TABLE tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data in SQLite table\n",
    "# storing this data is important since this is an unofficial API\n",
    "# every time I access it, I am risking not being able to access it again\n",
    "\n",
    "for lst in final_tweet_list:\n",
    "    for tweet in lst:\n",
    "        print(tweet)\n",
    "        c.execute(\"INSERT INTO tweets VALUES (?,?,?,?,?,?,?,?,?,?)\", tweet)\n",
    "        conn.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
