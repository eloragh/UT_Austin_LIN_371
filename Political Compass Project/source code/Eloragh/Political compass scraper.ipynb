{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Political Compass Scraper\n",
    "\n",
    "This file is what we used to scrape tweet IDs and the tweet data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit import Client\n",
    "from twikit import TwitterException \n",
    "from twikit import TooManyRequests\n",
    "from twikit.utils import Endpoint\n",
    "from twikit import BadRequest\n",
    "from requests import ReadTimeout\n",
    "from twikit import Unauthorized\n",
    "from translate import Translator\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "import ast\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this API requires authentication\n",
    "f = open('auth.txt', 'r')\n",
    "auth = f.read()\n",
    "f.close()\n",
    "auth_token = auth.split(\"\\n\")\n",
    "\n",
    "# don't hardcode your email and password into something!!!\n",
    "# the auth is in gitignore so I won't get hacked\n",
    "username = str(auth_token[0])\n",
    "email = str(auth_token[1])\n",
    "password = str(auth_token[2])\n",
    "\n",
    "# Initialize client\n",
    "client = Client(language='en-US', http2=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authentication(username, email, password):\n",
    "    try:\n",
    "        # Login to the service with provided user credentials\n",
    "        client.login(\n",
    "            auth_info_1=username ,\n",
    "            auth_info_2=email,\n",
    "            password=password)\n",
    "\n",
    "        print(\"Login successful!\")\n",
    "        return True\n",
    "\n",
    "    except BadRequest:\n",
    "        print(\"Login unsuccessful. One or more login parameters is incorrect.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authentication(username, email, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter LOVES to ban people when they log in repeatedly\n",
    "# saving the cookies makes sure I don't get banned (often)\n",
    "\n",
    "client.get_cookies()\n",
    "client.save_cookies('cookies.json')\n",
    "with open('cookies.json', 'r', encoding='UTF8') as f:\n",
    "    client.set_cookies(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Housekeeping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housekeeping function\n",
    "# each different method uses a different API endpoint\n",
    "# each different API endpoint has a rate limit\n",
    "# you can hit it a certain number of times per a time period (usually 15 minutes)\n",
    "# this tells me how much time I have left if I've hit the rate limit\n",
    "\n",
    "def get_limit_reset_time(endpoint: str):\n",
    "    res = requests.get(\n",
    "        endpoint,\n",
    "        headers=client._base_headers,\n",
    "        cookies=client.get_cookies()\n",
    "    )\n",
    "    return ceil(int(res.headers['x-rate-limit-reset']) - time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rate_limit_search_tweet():\n",
    "    try:\n",
    "        print(client.search_tweet(\n",
    "            f'from:JoeBiden since:2020-01-01 until:2021-03-01', 'Latest', count=40\n",
    "        ))\n",
    "        return True\n",
    "    except TooManyRequests:\n",
    "        reset_time = get_limit_reset_time(Endpoint.USER_TWEETS)\n",
    "        print(f'rate limit is reset after {reset_time} seconds.')\n",
    "        return False\n",
    "\n",
    "get_rate_limit_search_tweet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rate_limit_tweet_by_id():\n",
    "    try:\n",
    "        print(client.get_tweet_by_id(1351951465674276869))\n",
    "    except TooManyRequests:\n",
    "        reset_time = get_limit_reset_time(Endpoint.USER_TWEETS)\n",
    "        print(f'rate limit is reset after {reset_time} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another housekeeping function\n",
    "# if I'm suddenly getting 403 errors, I can use this to check if I've been banned\n",
    "# sometimes I just have to go on the browser and reauthenticate\n",
    "\n",
    "def check_user_status(user_id):\n",
    "    \"\"\"\n",
    "    True if the user is active, otherwise false (not exists or suspended).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.get_user_by_id(user_id)\n",
    "    except TwitterException as e:\n",
    "        if str(e).startswith('Invalid user id'):\n",
    "            return False\n",
    "        raise e\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "check_user_status(1783351954372005890)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: the user handle, a beginning and end of a date range\n",
    "# OUTPUT: the user handle, the user_ID, and the scraped tweets\n",
    "def get_all_tweets(handle, since, until):\n",
    "\n",
    "    try:\n",
    "        # load the cookies so you don't login a million times and get banned\n",
    "        client.load_cookies('cookies.json')\n",
    "\n",
    "        # initialize the list we will store our data in\n",
    "        mass_tweets = []\n",
    "\n",
    "        since = f'{since}-01-01'\n",
    "        until = f'{until}-01-31'\n",
    "\n",
    "        # this will pull the first forty tweets\n",
    "        tweets = client.search_tweet(\n",
    "            f'from:{handle} since:{since} until:{until}', 'Top'\n",
    "        )\n",
    "        tweets1 = [tweet.id for tweet in tweets]\n",
    "        mass_tweets += tweets1\n",
    "\n",
    "        # this endpoint has a rate limit of 50 hits per 15 minutes\n",
    "        # 15 min = 900 seconds\n",
    "        # 900//50 = 18\n",
    "        # allows the program to be automated\n",
    "        time.sleep(18)\n",
    "        \n",
    "        # if it returns an empty list, the user had no available tweets during the date time range\n",
    "        if len(tweets) == 0:\n",
    "            return([])\n",
    "        \n",
    "        # this will keep looking for tweets until a certain number of them has been reached\n",
    "        while len(tweets) > 0 and len(mass_tweets) < 30:\n",
    "\n",
    "            # this API provides a 'tweet' object, but we only want the id when we return\n",
    "            tweets = tweets.next()\n",
    "            tweets1 = [tweet.id for tweet in tweets]\n",
    "            mass_tweets += tweets1\n",
    "            time.sleep(18) # cooldown\n",
    "\n",
    "            # keep pulling tweets until number is hit or there are none left\n",
    "\n",
    "            # we need to make a check in case we've hit the max number of tweets we can scrape\n",
    "            # this prevents us from pinging the API for no reason\n",
    "            if len(tweets) == 0:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    except ReadTimeout:\n",
    "        return(mass_tweets)\n",
    "    except:\n",
    "        return(mass_tweets)\n",
    "\n",
    "    return(mass_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data in SQLite table\n",
    "# storing this data is important since this is an unofficial API\n",
    "# every time I access it, I am risking not being able to access it again\n",
    "\n",
    "def insert_into_database(tweet):\n",
    "\n",
    "    conn = sqlite3.connect('tweets.db')\n",
    "    c = conn.cursor()\n",
    "\n",
    "    c.execute(\"INSERT INTO politician_tweets VALUES (?,?,?,?,?,?,?,?,?,?)\", tweet)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(handle, user_id, name, tweet_ids, since):\n",
    "    # load the cookies so you don't login a million times and get banned\n",
    "    client.load_cookies('cookies.json')\n",
    "\n",
    "    # initialize a list to store all tuples\n",
    "    i = 0\n",
    "\n",
    "    for tweet_id in tweet_ids:\n",
    "        try:\n",
    "            # using the IDs we pulled from above\n",
    "            tweet = client.get_tweet_by_id(tweet_id)\n",
    "\n",
    "            tweet_info = [int(tweet.id), int(user_id), name, handle, str(tweet.text), str(tweet.lang), 'False', 'null', str(tweet.created_at_datetime), str(since)]\n",
    "            insert_into_database(tweet_info)\n",
    "            i+=1\n",
    "\n",
    "            # this endpoint can process 150 tweets per 15 minutes\n",
    "            # 15 min = 900 seconds\n",
    "            # 900//150 = 6\n",
    "            # allows program to be fully automated\n",
    "            time.sleep(6)\n",
    "\n",
    "        # it throws an Index Error if the tweet has been deleted/ is not available\n",
    "        except IndexError:\n",
    "            print(f'Index Error: unable to process {tweet_id} from {name}')\n",
    "            continue\n",
    "        except ReadTimeout:\n",
    "            print('Read timeout')\n",
    "            continue\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_tweet_ids(user_list):\n",
    "    tweet_ids_list = []\n",
    "    no_tweets_list = []\n",
    "\n",
    "    for idx, user in enumerate(user_list):\n",
    "\n",
    "        try:\n",
    "            # all of our parameters for the function\n",
    "            user_id = user[0]\n",
    "            name = user[1]\n",
    "            handle = user[2]\n",
    "            since = user[3]\n",
    "            until = user[3] + 1\n",
    "\n",
    "            print(name)\n",
    "\n",
    "            # pull tweet IDs that we will user in second while loop\n",
    "            tweet_ids = get_all_tweets(handle, since=since, until=until)\n",
    "            print(f'{len(tweet_ids)} tweets collected for {name} for {since} election')\n",
    "\n",
    "            # we only add to the counter if that person actually had tweets to process\n",
    "            if len(tweet_ids) > 0:\n",
    "                tweet_ids_list.append([handle, user_id, name, tweet_ids, since])\n",
    "            \n",
    "            # I want to keep track of which politicians didn't tweet during their election year\n",
    "            else:\n",
    "                no_tweets_list.append([user_id, name, handle, since])\n",
    "\n",
    "        # this error happens when we try to hit the API too many times\n",
    "        except TooManyRequests:\n",
    "            print(\"Too many requests\")\n",
    "            print(get_rate_limit_search_tweet())\n",
    "            #time.sleep(900)\n",
    "        \n",
    "        # I honestly don't know why this error happens\n",
    "        # I'm too speedy for the requests module I guess\n",
    "        except ReadTimeout:\n",
    "            print(\"\"\"\"The read operation timed out.\n",
    "                      If authentication fails, you may be blocked or need to authenticate through a browser.\"\"\")\n",
    "            if authentication(username, email, password):\n",
    "                continue\n",
    "            elif not check_user_status(1547081484695216130):\n",
    "                print(\"Authentication failed. Function pull_tweet_ids terminating.\")\n",
    "                return idx, (tweet_ids_list, no_tweets_list)\n",
    "            else:\n",
    "                print(\"Unknown authentication issue. Function pull_tweet_ids terminating.\")\n",
    "                return idx, (tweet_ids_list, no_tweets_list)\n",
    "        \n",
    "        # elon musk caught my scent :(\n",
    "        # reauthenticate in a browser\n",
    "        except Unauthorized:\n",
    "            if authentication(username, email, password):\n",
    "                continue\n",
    "            else:\n",
    "                print(\"You need to reauthenticate through a browser.\")\n",
    "                return idx, (tweet_ids_list, no_tweets_list)\n",
    "        \n",
    "        except IndexError:\n",
    "            print(f'Index Error: unable to process tweets from {name}')\n",
    "            return idx, (tweet_ids_list, no_tweets_list)\n",
    "        \n",
    "        except:\n",
    "            print('Exception raised in function pull_tweet_ids')\n",
    "            return idx, (tweet_ids_list, no_tweets_list)\n",
    "\n",
    "    print(f'This program was able to find tweets for {len(tweet_ids_list)} out of {len(user_list)} politicians.')\n",
    "    print(f'{len(no_tweets_list)} politicians had no tweets during one or more of their campaign years.')\n",
    "    return tweet_ids_list, no_tweets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pulled_tweet_ids(tweet_ids_list, num_tweets):\n",
    "\n",
    "    for idx, user in enumerate(tweet_ids_list):\n",
    "        try:\n",
    "\n",
    "            # all the parameters we need\n",
    "            handle = user[0]\n",
    "            user_id = user[1]\n",
    "            name = user[2]\n",
    "            tweet_ids = user[3]\n",
    "            year = user[4]\n",
    "            \n",
    "            # we only want to do up to 25 tweets per politician\n",
    "            # this means we can process 6 politicians per rate timeout\n",
    "            # 150 tweets per 15 minutes\n",
    "            # not great but it's free\n",
    "\n",
    "            if len(tweet_ids) > num_tweets:\n",
    "                # random sample in an attempt to stay unbiased\n",
    "                tweet_ids = random.sample(tweet_ids, num_tweets)\n",
    "            \n",
    "            # process the tweets and add them to our holding list from above\n",
    "            tweets = process_tweets(handle, user_id, name, tweet_ids, year)\n",
    "            print(f\"{tweets}/{num_tweets} tweets processed for {name}\")\n",
    "        \n",
    "        except TooManyRequests:\n",
    "            print(\"Too many requests\")\n",
    "            print(get_rate_limit_tweet_by_id())\n",
    "            #time.sleep(900)\n",
    "\n",
    "        except ReadTimeout:\n",
    "            print(\"\"\"\"The read operation timed out.\n",
    "                      If authentication fails, you may be blocked or need to authenticate through a browser.\"\"\")\n",
    "            if authentication(username, email, password):\n",
    "                continue\n",
    "            elif not check_user_status(1547081484695216130):\n",
    "                print(\"Authentication failed. Function pull_tweet_ids terminating.\")\n",
    "                return idx\n",
    "            else:\n",
    "                print(\"Unknown authentication issue. Function pull_tweet_ids terminating.\")\n",
    "                return idx\n",
    "    \n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull and process data\n",
    "\n",
    "This is an automated process. It can run for hours (if there are no errors (there were a lot of errors! not code errors, API errors :( ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('tweets.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"\"\" SELECT twitter_user_id, politician_name, twitter_handle, election_year\n",
    "            FROM coordinates\n",
    "            WHERE twitter_active_during_election = 'True'\n",
    "            \"\"\")\n",
    "#active_user_list = c.fetchall()\n",
    "#print(active_user_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_ids, no_tweets = pull_tweet_ids(user_list)\n",
    "c.execute(\"\"\"SELECT DISTINCT *\n",
    "          FROM temporary\"\"\")\n",
    "tweet_ids = c.fetchall()\n",
    "tweet_ids = tweet_ids[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweet_list = []\n",
    "\n",
    "for id in tweet_ids:\n",
    "    new_tweet_list.append([x for x in id])\n",
    "\n",
    "for item in new_tweet_list:\n",
    "    item[3] = ast.literal_eval(item[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_pulled_tweet_ids(new_tweet_list, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute(\"\"\"SELECT DISTINCT tweet_id,\n",
    "          user_id,\n",
    "          user_name,\n",
    "          user_handle,\n",
    "          tweet_text,\n",
    "          tweet_original_lang,\n",
    "          created_date,\n",
    "          election_year\n",
    "          FROM politician_tweets \"\"\")\n",
    "tweets = c.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets, columns=['tweet id', 'user id', 'politician name', 'twitter handle', 'tweet text', 'tweet original lang', 'tweet created', 'election'])\n",
    "df.to_csv('tweet_data_iteration2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
