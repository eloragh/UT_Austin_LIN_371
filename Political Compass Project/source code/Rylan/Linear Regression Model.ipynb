{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eaesp\\AppData\\Local\\Temp\\ipykernel_27492\\2490293955.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\eaesp\\OneDrive\\Documents\\GitHub\\LIN-371\\Political Compass Project\\data\\iteration 1 tweet data.csv', index_col=0)\n",
    "df = pd.DataFrame(data)\n",
    "tweets = df['tweet text']\n",
    "x_labels = df['x coordinate']\n",
    "y_labels = df['y coordinate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split 20% of the data into testing and validation\n",
    "x_sentences_train, x_sentences_test_val, x_y_train, x_y_test_val = train_test_split(tweets, x_labels, test_size=0.2, random_state=1000)\n",
    "# Then split 50% of the test+val data as validation\n",
    "x_sentences_test, x_sentences_val, x_test, x_val = train_test_split(x_sentences_test_val, x_test_val, test_size=0.5, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split 20% of the data into testing and validation\n",
    "y_sentences_train, y_sentences_test_val, y_train, y_test_val = train_test_split(tweets, y_labels, test_size=0.2, random_state=1000)\n",
    "# Then split 50% of the test+val data as validation\n",
    "y_sentences_test, y_sentences_val, y_test, y_val = train_test_split(x_sentences_test_val, x_test_val, test_size=0.5, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3236,)\n",
      "(405,)\n",
      "(404,)\n",
      "(3236,)\n",
      "(405,)\n",
      "(404,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "x_vectorizer = CountVectorizer()\n",
    "x_vectorizer.fit(x_sentences_train) #in the past we did fit_transform()\n",
    "X_train = x_vectorizer.transform(x_sentences_train)\n",
    "X_test  = x_vectorizer.transform(x_sentences_test)\n",
    "X_val = x_vectorizer.transform(x_sentences_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vectorizer = CountVectorizer()\n",
    "y_vectorizer.fit(x_sentences_train) #in the past we did fit_transform()\n",
    "Y_train = y_vectorizer.transform(y_sentences_train)\n",
    "Y_test  = y_vectorizer.transform(y_sentences_test)\n",
    "Y_val = y_vectorizer.transform(y_sentences_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3236, 16663)\n",
      "(3236, 16663)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: -0.1565791771838565\n"
     ]
    }
   ],
   "source": [
    "classifier = LinearRegression()\n",
    "classifier.fit(X_train, x_train)\n",
    "score = classifier.score(X_test, x_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: -0.31029947632437405\n"
     ]
    }
   ],
   "source": [
    "classifier = LinearRegression()\n",
    "classifier.fit(Y_train, y_train)\n",
    "score = classifier.score(Y_test, y_test)\n",
    "print(\"Accuracy:\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
