{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qaa_KNCEekgk"
      },
      "source": [
        "# Building Neural Networks\n",
        "\n",
        "LIN 371 :: UT Austin\n",
        "\n",
        "Jessy Li\n",
        "\n",
        "Most of the material adapted from https://realpython.com/python-keras-text-classification/\n",
        "\n",
        "**NOTE**: the original post has an **error** in that they used the test data as validation data during training. In practice, ALWAYS use a separate validation set! Validation data is used to tune the model with respect to some parameters, as we will show see below.\n",
        "\n",
        "In this tutorial we'll explore:\n",
        "* Using word embeddings in Keras\n",
        "* Using RNN (e.g., LSTM) in Keras\n",
        "* Applying Dropout regularization\n",
        "\n",
        "## Data\n",
        "We will again use sentiment analysis as an example.\n",
        "\n",
        "Go ahead and download the data set from [the Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences) from the UCI Machine Learning Repository.\n",
        "\n",
        "By the way, this repository is a wonderful source for machine learning data sets when you want to try out some algorithms. This data set includes labeled reviews from IMDb, Amazon, and Yelp. Each review is marked with a score of 0 for a negative sentiment or 1 for a positive sentiment.\n",
        "\n",
        "Extract the folder into the current directory (if using Colab: upload to your Google Drive) and go ahead and load the data with Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gxDPM2ngnRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92dca45c-66c8-4bc0-bbbb-d4238e4ccec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqNSqj7aekgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d919e1-d5f9-4813-906d-daa79818f477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            sentence  label source\n",
            "0                           Wow... Loved this place.      1   yelp\n",
            "1                                 Crust is not good.      0   yelp\n",
            "2          Not tasty and the texture was just nasty.      0   yelp\n",
            "3  Stopped by during the late May bank holiday of...      1   yelp\n",
            "4  The selection on the menu was great and so wer...      1   yelp\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filepath_dict = {'yelp':   '/content/drive/My Drive/LIN371/sentiment-labelled-sentences/yelp_labelled.txt',\n",
        "                 'amazon': '/content/drive/My Drive/LIN371/sentiment-labelled-sentences/amazon_cells_labelled.txt',\n",
        "                 'imdb':   '/content/drive/My Drive/LIN371/sentiment-labelled-sentences/imdb_labelled.txt'}\n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9ldNImBekgm"
      },
      "source": [
        "Now split the data into **train/validation/test**. Usually, with a moderately-sized dataset like this one, a rough 80-20 split for training-testing is good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODizqutuekgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e934fa6-7ae6-4bdb-cf91-d0a406a2d3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2198,)\n",
            "(275,)\n",
            "(275,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentences = df['sentence'].values\n",
        "y = df['label'].values\n",
        "# First split 20% of the data into testing and validation\n",
        "sentences_train, sentences_test_val, y_train, y_test_val = train_test_split(sentences, y, test_size=0.2, random_state=1000)\n",
        "# Then split 50% of the test+val data as validation\n",
        "sentences_test, sentences_val, y_test, y_val = train_test_split(sentences_test_val, y_test_val, test_size=0.5, random_state=1000)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YFx5V9bekgn"
      },
      "source": [
        "Transform our sentences into one-hot representations using CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNCujvb2ekgn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(sentences_train) #in the past we did fit_transform()\n",
        "X_train = vectorizer.transform(sentences_train)\n",
        "X_test  = vectorizer.transform(sentences_test)\n",
        "X_val = vectorizer.transform(sentences_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "s8GjGZ1FP2tt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba6c9d8-fa7f-4981-dbe7-cb5fb4ea8427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2198, 4642)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my8KNJgiekgo"
      },
      "source": [
        "## Building a logistic regression benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFmZm7ZOekgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc9bb7e-32ef-435e-8e73-04ec8b05266b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8472727272727273\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "score = classifier.score(X_test, y_test)\n",
        "print(\"Accuracy:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXDeQyewekgo"
      },
      "source": [
        "## Keras basics\n",
        "\n",
        "[Keras](https://keras.io/) is a popular, high-level deep learning and neural networks API by [François Chollet](https://twitter.com/fchollet) which is capable of running on top of [Tensorflow](https://www.tensorflow.org/) (Google).\n",
        "\n",
        "To quote the wonderful book by François Chollet, Deep Learning with Python:\n",
        "\n",
        "\n",
        "_Keras is a model-level library, providing high-level building blocks for developing deep-learning models. It doesn’t handle low-level operations such as tensor manipulation and differentiation. Instead, it relies on a specialized, well-optimized tensor library to do so, serving as the backend engine of Keras_\n",
        "\n",
        "\n",
        "It is a great way to start experimenting with neural networks without having to implement every layer and piece on your own. For example Tensorflow is a great machine learning library, but you have to implement a lot of boilerplate code to have a model running.\n",
        "\n",
        "**Note**: a very, very popular deep learning framework is [PyTorch](https://pytorch.org/); it is extremely powerful but also less _high level_ than Keras. In class, we work with Keras because it is very, very easy to understand.\n",
        "\n",
        "### Installing Keras\n",
        "\n",
        "**You do not need to do this if you're using COLAB**\n",
        "\n",
        "Two ways (among many) to install:\n",
        "* You can install Keras using the Anaconda Navigator; serach for \"keras\". This **will not** install Tensorflow as your backend so you must do it separately by yourself, in the same interface.\n",
        "* You can also install it using pip, but you will need to install the backend, e.g., Tensorflow, yourself:\n",
        "```\n",
        "pip install tensorflow\n",
        "pip install keras\n",
        "```\n",
        "\n",
        "### Your First Keras Model\n",
        "\n",
        "The most convenient way to think of a Keras model is a stack of layers; in Keras this is handled by [the Sequential Model API](https://keras.io/models/sequential/).\n",
        "\n",
        "The Sequential model is a linear stack of layers, where you can use the large variety of available layers in Keras. The most common layer is the `Dense` layer which is your regular densely connected neural network layer with all the weights and biases that you are already familiar with.\n",
        "\n",
        "Before we build our model, we need to know the input dimension of our feature vectors. This happens only in the first layer since the following layers can do automatic shape inference. In order to build the Sequential model, you can add layers one by one in order as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOQRwvMUekgp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "#TODO\n",
        "input_dim = X_train.shape[1]\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim = input_dim, activation = \"relu\"))\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El569ZCQekgp"
      },
      "source": [
        "Before you can start with the training of the model, you need to configure the learning process. This is done with the `.compile()` method. This method specifies the optimizer and the loss function.\n",
        "\n",
        "Additionally, you can add a list of metrics which can be later used for evaluation, but they do not influence the training. In this case, we want to use the binary cross entropy and the Adam optimizer (a popular method that's often used instead of SGD).\n",
        "\n",
        "Keras also includes a handy `.summary()` function to give an overview of the model and the number of parameters available for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k7EpcOKekgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9194ff6-03b5-49ac-eaca-a4536690f116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 10)                46430     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 46441 (181.41 KB)\n",
            "Trainable params: 46441 (181.41 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9Xvz0ZCekgp"
      },
      "source": [
        "Note the ``Param #`` in the table --- this specifies the number of parameters (w's) in each layer. Always a good idea to check the dimensions!\n",
        "\n",
        "Since the training in neural networks is an iterative process, the training won’t just stop after it is done. You have to specify the number of iterations you want the model to be training. Those **completed iterations** are commonly called epochs. We want to run it for 20 epochs to be able to see how the training loss and accuracy are changing after each epoch.\n",
        "\n",
        "Another parameter you have to your selection is the batch size. The batch size is responsible for how many samples we want to use in **one forward/backward pass** (think of that as the number of examples in each iteration in Stochastic Gradient Decesnt). This increases the speed of the computation as it need fewer epochs to run, but it also needs more memory, and the model may degrade with larger batch sizes. Since we have a small training set, we can leave this to a low batch size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFFDWghCekgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f873f1-d8cb-4bf2-b107-1cdaba4294f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "138/138 [==============================] - 3s 12ms/step - loss: 0.6792 - accuracy: 0.6328 - val_loss: 0.6511 - val_accuracy: 0.7382\n",
            "Epoch 2/10\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.5641 - accuracy: 0.8480 - val_loss: 0.5561 - val_accuracy: 0.7636\n",
            "Epoch 3/10\n",
            "138/138 [==============================] - 1s 6ms/step - loss: 0.4173 - accuracy: 0.9095 - val_loss: 0.4912 - val_accuracy: 0.8073\n",
            "Epoch 4/10\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.3088 - accuracy: 0.9436 - val_loss: 0.4516 - val_accuracy: 0.8218\n",
            "Epoch 5/10\n",
            "138/138 [==============================] - 1s 10ms/step - loss: 0.2338 - accuracy: 0.9627 - val_loss: 0.4317 - val_accuracy: 0.8109\n",
            "Epoch 6/10\n",
            "138/138 [==============================] - 1s 5ms/step - loss: 0.1830 - accuracy: 0.9686 - val_loss: 0.4198 - val_accuracy: 0.8182\n",
            "Epoch 7/10\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.1465 - accuracy: 0.9741 - val_loss: 0.4174 - val_accuracy: 0.8182\n",
            "Epoch 8/10\n",
            "138/138 [==============================] - 2s 13ms/step - loss: 0.1194 - accuracy: 0.9818 - val_loss: 0.4236 - val_accuracy: 0.8182\n",
            "Epoch 9/10\n",
            "138/138 [==============================] - 1s 6ms/step - loss: 0.0992 - accuracy: 0.9836 - val_loss: 0.4221 - val_accuracy: 0.8255\n",
            "Epoch 10/10\n",
            "138/138 [==============================] - 1s 6ms/step - loss: 0.0834 - accuracy: 0.9886 - val_loss: 0.4299 - val_accuracy: 0.8145\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e173edc3a00>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#TODO\n",
        "model.fit(X_train, y_train,\n",
        "          epochs = 10,\n",
        "          batch_size = 16,\n",
        "          validation_data = (X_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdwI4BvKekgq"
      },
      "source": [
        "Note that if you rerun the `.fit()` method, you’ll start off with the computed weights from the previous training. Make sure to compile the model again before you start training the model again.\n",
        "\n",
        "Now you can use the `.evaluate()` method to measure the accuracy of the model. We expect that the training data has a higher accuracy then for the testing data. The longer you would train a neural network, the more likely it is that it starts overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNmvhRjaekgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8936a639-f870-4068-90c0-7f16157fc737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3879 - accuracy: 0.8436\n",
            "Testing accuracy: 0.8436\n"
          ]
        }
      ],
      "source": [
        "#TODO\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Testing accuracy: {:.4f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uaotk9Q6ekgr"
      },
      "source": [
        "Did we overfit?\n",
        "\n",
        "### Using word embeddings\n",
        "\n",
        "How can you get such a word embedding? You have two options for this:\n",
        "* Train your word embeddings during the training of your neural network.\n",
        "* Use pretrained word embeddings (e.g., Word2Vec, Glove), which you can directly use in your model. There you have the option to either leave these word embeddings unchanged during training or you train them also.\n",
        "\n",
        "Now you need to tokenize the data into a format that can be used by the word embeddings. Keras offers a couple of convenience methods for [text preprocessing](https://keras.io/preprocessing/text/) and [sequence preprocessing](https://keras.io/preprocessing/sequence/) which you can employ to prepare your text.\n",
        "\n",
        "You can start by using the `Tokenizer` utility class which can vectorize a text corpus into a list of integers. Each integer maps to a value in a dictionary that encodes the entire corpus, with the keys in the dictionary being the vocabulary terms themselves.\n",
        "\n",
        "You can add the parameter `num_words`, which is responsible for setting the size of the vocabulary. The most common `num_words` words will be then kept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEdh1Y18ekgs"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#TODO\n",
        "tokenizer = Tokenizer(num_words = 10000)\n",
        "tokenizer.fit_on_texts(sentences_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
        "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
        "X_val = tokenizer.texts_to_sequences(sentences_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7_ngbBoekgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84bddcb2-87c0-4a95-e759-779f2309dae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an earlier review noted, plug in this charger and nothing happens.\n",
            "[26, 47, 773, 563, 1974, 339, 11, 8, 237, 2, 174, 1975]\n"
          ]
        }
      ],
      "source": [
        "print(sentences_train[2])\n",
        "print(X_train[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIILJpZ5ekgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5f54d7-3bfc-4e25-c2c8-ce6890d240e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I went on Motorola's website and followed all directions, but could not get it to pair again.\n",
            "[3, 227, 19, 1976, 774, 2, 1977, 32, 1978, 22, 111, 13, 81, 6, 7, 775, 103]\n"
          ]
        }
      ],
      "source": [
        "print(sentences_train[3])\n",
        "print(X_train[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZFseuUfekgt"
      },
      "source": [
        "The indexing is ordered after the most common words in the text, which you can see by the word `the` having the index 1.\n",
        "\n",
        "It is important to note that the index 0 is reserved and is not assigned to any word. This zero index is used for *padding*, which I’ll introduce in a moment.\n",
        "\n",
        "You can see the index of each word by taking a look at the word_index dictionary of the Tokenizer object.\n",
        "\n",
        "Unknown words (words that are not in the vocabulary) are denoted in Keras with `word_count + 1` since they can also hold some information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJD7D5XZekgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2080109d-21bb-4716-f78c-5dc4fd903028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the: 1\n",
            "all: 32\n",
            "happy: 202\n",
            "sad: 671\n"
          ]
        }
      ],
      "source": [
        "for word in ['the', 'all', 'happy', 'sad']:\n",
        "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaS1juteekgu"
      },
      "source": [
        "**Note**: Pay close attention to the difference between this technique and the `X_train` that was produced by scikit-learn’s `CountVectorizer`. With `CountVectorizer`, we had stacked vectors of word counts, and each vector was the **same** length (the size of the total corpus vocabulary). With Keras `Tokenizer`, the resulting vectors equal the length of each text/sentence, and the numbers don’t denote counts, but rather correspond to the word values from the dictionary `tokenizer.word_index`.\n",
        "\n",
        "This means that one problem that we have is that each text sequence has in most cases **different** length of words. To counter this, you can use `pad_sequence()` which simply pads the sequence of words with zeros. By default, it prepends zeros but we can also append them. Typically it does not matter whether you prepend or append zeros.\n",
        "\n",
        "Additionally you would want to add a `maxlen` parameter to specify how long the sequences should be. This cuts sequences that exceed that number. In the following code, you can see how to pad sequences with Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVNJDFhqekgu"
      },
      "outputs": [],
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "maxlen = 100\n",
        "X_train = pad_sequences(X_train, maxlen = maxlen, padding = \"post\")\n",
        "X_test = pad_sequences(X_test, maxlen = maxlen, padding = \"post\")\n",
        "X_val = pad_sequences(X_val, maxlen = maxlen, padding = \"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd5PoNesekgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b5c0c3-f25e-49a9-8a82-c4d436b8fc5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an earlier review noted, plug in this charger and nothing happens.\n",
            "[  26   47  773  563 1974  339   11    8  237    2  174 1975    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ]
        }
      ],
      "source": [
        "print(sentences_train[2])\n",
        "print(X_train[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiY4Gw4-ekgv"
      },
      "source": [
        "The first values represent the index in the vocabulary as you have learned from the previous examples. You can also see that the resulting feature vector contains mostly zeros, since you have a fairly short sentence.\n",
        "\n",
        "Let's again take a look at the 4th example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vOOonSGekgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d62d86-0178-409a-d653-551bc4a71e5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All of the tapas dishes were delicious!\n",
            "[  32    9    1 1979  956   43  215    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ]
        }
      ],
      "source": [
        "print(sentences_train[4])\n",
        "print(X_train[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XIGHZ8Oekgv"
      },
      "source": [
        "### Keras Embedding Layer\n",
        "\n",
        "Now we are ready to learn a new embedding space through a task, just like we did with neural langauge models or Word2Vec. In this case, our task is sentiment classification. The first step is to use Keras' [Embedding Layer](https://keras.io/layers/embeddings/) which takes the one-hot integers and maps them to a dense vector of the embedding, randomly initialized. You will need the following parameters:\n",
        "* `input_dim`: the size of the vocabulary\n",
        "* `output_dim`: the size of the dense vector\n",
        "* `input_length`: the length of the sequence\n",
        "\n",
        "But how do we go from the embedding layer, which gives a `num_input_example` X `sequence_length` matrix to a dense layer, which expects a flat vector? One way is to just average the embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfpxKp6iekgv",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1e73b3-5898-4976-fddc-1f4c07bd92f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 50)           237150    \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 50)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 237671 (928.40 KB)\n",
            "Trainable params: 237671 (928.40 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.Embedding(input_dim = len(tokenizer.word_index)+1,\n",
        "                           output_dim = embedding_dim,\n",
        "                           input_length = maxlen))\n",
        "\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(10, activation = \"relu\"))\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7nUG231ekgv"
      },
      "source": [
        "Now we have maaaannnyyyy parameters to train...\n",
        "\n",
        "Let's look at the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I32m7YsMekgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4baa2152-1faf-4201-e6a2-20242a867e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "138/138 [==============================] - 6s 23ms/step - loss: 0.6928 - accuracy: 0.5136 - val_loss: 0.6918 - val_accuracy: 0.4982\n",
            "Epoch 2/10\n",
            "138/138 [==============================] - 2s 14ms/step - loss: 0.6882 - accuracy: 0.5605 - val_loss: 0.6857 - val_accuracy: 0.5527\n",
            "Epoch 3/10\n",
            "138/138 [==============================] - 2s 17ms/step - loss: 0.6702 - accuracy: 0.6015 - val_loss: 0.6622 - val_accuracy: 0.7236\n",
            "Epoch 4/10\n",
            "138/138 [==============================] - 3s 18ms/step - loss: 0.6077 - accuracy: 0.7953 - val_loss: 0.6035 - val_accuracy: 0.7527\n",
            "Epoch 5/10\n",
            "138/138 [==============================] - 3s 22ms/step - loss: 0.5031 - accuracy: 0.8553 - val_loss: 0.5349 - val_accuracy: 0.7964\n",
            "Epoch 6/10\n",
            "138/138 [==============================] - 3s 21ms/step - loss: 0.3968 - accuracy: 0.8963 - val_loss: 0.4868 - val_accuracy: 0.8109\n",
            "Epoch 7/10\n",
            "138/138 [==============================] - 4s 29ms/step - loss: 0.3151 - accuracy: 0.9249 - val_loss: 0.4586 - val_accuracy: 0.8000\n",
            "Epoch 8/10\n",
            "138/138 [==============================] - 3s 20ms/step - loss: 0.2531 - accuracy: 0.9427 - val_loss: 0.4815 - val_accuracy: 0.7636\n",
            "Epoch 9/10\n",
            "138/138 [==============================] - 3s 23ms/step - loss: 0.2088 - accuracy: 0.9554 - val_loss: 0.4678 - val_accuracy: 0.7782\n",
            "Epoch 10/10\n",
            "138/138 [==============================] - 3s 23ms/step - loss: 0.1781 - accuracy: 0.9586 - val_loss: 0.4402 - val_accuracy: 0.7927\n",
            "Testing Accuracy:  0.8436\n"
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs = 10, validation_data = (X_val, y_val), batch_size = 16)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA1oFNTLekgw"
      },
      "source": [
        "### Using Pretrained Word Embeddings\n",
        "\n",
        "We just saw an example of jointly learning word embeddings incorporated into the larger model that we want to solve. But our data is too small compared to the number of parameters we have to learn and we're grossly overfitting. One way to solve this is to use pre-trained word embeddings.\n",
        "\n",
        "We will work with the [GloVe](https://nlp.stanford.edu/projects/glove/) (Global Vectors for Word Representation) word embeddings from the Stanford NLP Group as their size is more manageable than the Word2Vec word embeddings provided by Google. Go ahead and download the 6B (trained on 6 billion words) word embeddings from [here](http://nlp.stanford.edu/data/glove.6B.zip) (822 MB).\n",
        "\n",
        "This is a large file with 400000 lines, with each line representing a word followed by its vector as a stream of floats. For example, here are the first 50 characters of the first line:\n",
        "```\n",
        "Shell:\n",
        "$ head -n 1 data/glove_word_embeddings/glove.6B.50d.txt | cut -c-50\n",
        "    the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.04445\n",
        "```\n",
        "\n",
        "Now let's build a matrix, where the row id's correspond to the one-hots in our `Tokenizer`'s `word_index`, and the columns correspond to that word's embedding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxx5HgQ8ekgw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath) as f:\n",
        "      for line in f:\n",
        "        word, *vector = line.split()\n",
        "        if word in word_index:\n",
        "          idx = word_index[word]\n",
        "          embedding_matrix[idx] = np.array(vector, dtype = np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix('/content/drive/My Drive/LIN371/glove.6B.50d.txt',\n",
        "                                           tokenizer.word_index,\n",
        "                                           embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvsmoFSoekgw"
      },
      "source": [
        "We can check how many of the words we have in training are in Glove:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32pAu4O8ekgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64bc823-8931-4490-bd9b-e28e3a999360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9424414927261227\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index)+1\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "print(nonzero_elements / vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27jo_Fm6ekgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e415f3-9a78-4b74-aa39-a6f3310cb2e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 100, 50)           237150    \n",
            "                                                                 \n",
            " global_average_pooling1d_2  (None, 50)                0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 237671 (928.40 KB)\n",
            "Trainable params: 521 (2.04 KB)\n",
            "Non-trainable params: 237150 (926.37 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(layers.Embedding(input_dim = vocab_size,\n",
        "                           output_dim = embedding_dim,\n",
        "                           input_length = maxlen,\n",
        "                           weights = [embedding_matrix],trainable = False))\n",
        "\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(10, activation = \"relu\"))\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp_XW3SVekgx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1056dec9-b76a-4683-a37f-9990e4dae725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "138/138 [==============================] - 2s 5ms/step - loss: 0.6912 - accuracy: 0.5405 - val_loss: 0.6889 - val_accuracy: 0.5855\n",
            "Epoch 2/10\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6839 - accuracy: 0.5878 - val_loss: 0.6827 - val_accuracy: 0.6364\n",
            "Epoch 3/10\n",
            "138/138 [==============================] - 1s 5ms/step - loss: 0.6734 - accuracy: 0.6338 - val_loss: 0.6728 - val_accuracy: 0.6436\n",
            "Epoch 4/10\n",
            "138/138 [==============================] - 1s 7ms/step - loss: 0.6602 - accuracy: 0.6592 - val_loss: 0.6615 - val_accuracy: 0.6545\n",
            "Epoch 5/10\n",
            "138/138 [==============================] - 1s 6ms/step - loss: 0.6457 - accuracy: 0.6642 - val_loss: 0.6539 - val_accuracy: 0.6655\n",
            "Epoch 6/10\n",
            "138/138 [==============================] - 1s 7ms/step - loss: 0.6319 - accuracy: 0.6915 - val_loss: 0.6422 - val_accuracy: 0.6800\n",
            "Epoch 7/10\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.7138 - val_loss: 0.6295 - val_accuracy: 0.6982\n",
            "Epoch 8/10\n",
            "138/138 [==============================] - 1s 4ms/step - loss: 0.6063 - accuracy: 0.7197 - val_loss: 0.6206 - val_accuracy: 0.6982\n",
            "Epoch 9/10\n",
            "138/138 [==============================] - 0s 3ms/step - loss: 0.5969 - accuracy: 0.7247 - val_loss: 0.6141 - val_accuracy: 0.7018\n",
            "Epoch 10/10\n",
            "138/138 [==============================] - 0s 3ms/step - loss: 0.5869 - accuracy: 0.7329 - val_loss: 0.6129 - val_accuracy: 0.6945\n",
            "Testing Accuracy:  0.8436\n"
          ]
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs = 10, validation_data = (X_val, y_val), batch_size = 16)\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLc6giOHekgx"
      },
      "source": [
        "### LSTMs with Keras\n",
        "\n",
        "Recall that we were \"averaging\" the word embeddings in order to get a sentence representation... but can we do better? In class, we discussed the use of RNNs (e.g., LSTM) as a way of \"summarizing\" sentence meaning in its hidden vectors. As it turns out, it is very easy to use an LSTM instead of doing an average pooling; you only need to specify an output dimension for the LSTM, i.e., the size of your \"sentence summary vector\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz59qcRGekgx"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "#TODO\n",
        "\n",
        "model.add(layers.Embedding(input_dim = vocab_size,\n",
        "                           output_dim = embedding_dim,\n",
        "                           input_length = maxlen,\n",
        "                           weights = [embedding_matrix],trainable = False))\n",
        "\n",
        "model.add(layers.LSTM(64))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Dense(10, activation = \"relu\"))\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMAhgBI8ekgx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHf_F-0lekgx"
      },
      "source": [
        "## Notable hyperparameters\n",
        "\n",
        "You might already have noticed: there are so many hyperparameters associated with a neural network! How to find the best combination? Unfortuantely, in most cases, you'll just have to try varying one parameter and hold everything else consant. Name a few ones that we looked at in this tutorial!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRJ7-a9yekgx"
      },
      "source": [
        "* Dropout\n",
        "* Padding sent length\n",
        "* \\# of nodes\n",
        "* \\# epoches\n",
        "* \\# of layers\n",
        "* embedding dimension\n",
        "* activation\n",
        "* learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4pKGyN2ekgx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\", trainable=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}