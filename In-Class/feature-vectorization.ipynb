{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIN 373 UT Austin :: Jessy Li\n",
    "\n",
    "## Vectorizing categorical features\n",
    "\n",
    "### Review: the inner workings\n",
    "\n",
    "Let's encode the Naive Bayes example we used in class into the count table shown on the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's the data. Let's pretend these are grammatical sentences.\n",
    "docs_train = [\"Chinese Beijing Chinese\",\n",
    "              \"Chinese Chinese Shanghai\",\n",
    "              \"Chinese Macao\",\n",
    "             \"Tokyo Japan Chinese\"]\n",
    "Y_train = [1, 1, 1, 0]\n",
    "\n",
    "docs_test = [\"Chinese Chinese Chinese Tokyo Japan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chinese', 'Beijing', 'Chinese'], ['Chinese', 'Chinese', 'Shanghai'], ['Chinese', 'Macao'], ['Tokyo', 'Japan', 'Chinese']]\n",
      "[['Chinese', 'Chinese', 'Chinese', 'Tokyo', 'Japan']]\n"
     ]
    }
   ],
   "source": [
    "## first need to tokenize each document\n",
    "docs_train_tokenized = [doc.split() for doc in docs_train]\n",
    "print(docs_train_tokenized)\n",
    "\n",
    "docs_test_tokenized = [doc.split() for doc in docs_test]\n",
    "print(docs_test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we put words into a table?\n",
    "First, we need to create that table. The rows are just the examples. But we need to come up with the columns.\n",
    "We need to assign each word to a column number!\n",
    "We do that by creating a dictionary to map from word to a unique column id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chinese': 0, 'Beijing': 1, 'Shanghai': 2, 'Macao': 3, 'Tokyo': 4, 'Japan': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_col_id = {}\n",
    "for doc in docs_train_tokenized:\n",
    "    for word in doc:\n",
    "        if word not in word_to_col_id:\n",
    "            word_to_col_id[word] = len(word_to_col_id)\n",
    "            \n",
    "print(word_to_col_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make the table, and fill it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 1. 0. 0. 0. 0.]\n",
      " [2. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.zeros((len(docs_train), len(word_to_col_id)))\n",
    "for i,doc in enumerate(docs_train_tokenized):\n",
    "    for word in doc:\n",
    "        col_id = word_to_col_id[word]\n",
    "        X_train[i][col_id] += 1\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for testing docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.zeros((len(docs_test), len(word_to_col_id)))\n",
    "for i,doc in enumerate(docs_test_tokenized):\n",
    "    for word in doc:\n",
    "        col_id = word_to_col_id[word]\n",
    "        X_test[i][col_id] += 1\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 2: adding features\n",
    "\n",
    "What if we would like to add some new features? Say, the author of a document:\n",
    "```\n",
    "authors_train = ['Cao', 'Wang', 'Cao', 'Hirao']\n",
    "authors_test = ['Cao']\n",
    "```\n",
    "What are the high level steps we should take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "authors_train = ['Cao', 'Wang', 'Cao', 'Hirao']\n",
    "authors_test = ['Cao']\n",
    "\n",
    "## First, let's vectorize that feature!\n",
    "author_to_col_id = {}\n",
    "for author in authors_train:\n",
    "    if author not in author_to_col_id:\n",
    "        author_to_col_id[author] = len(author_to_col_id)\n",
    "## just one way to do this: create a new matrix for authors\n",
    "X_train_authors = np.zeros( (len(authors_train), len(author_to_col_id)) )\n",
    "for i, author in enumerate(authors_train):\n",
    "    X_train_authors[i][author_to_col_id[author]] += 1\n",
    "\n",
    "print(X_train_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [2. 0. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "## now merge the features\n",
    "X_train = np.hstack((X_train, X_train_authors))\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 0. 0. 0. 1. 1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## now do the same for test\n",
    "X_test_authors = np.zeros( (len(authors_test), len(author_to_col_id)) )\n",
    "for i, author in enumerate(authors_test):\n",
    "    if author in author_to_col_id:\n",
    "        X_test_authors[i][author_to_col_id[author]] += 1\n",
    "X_test_authors\n",
    "\n",
    "X_test = np.hstack((X_test, X_test_authors))\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a tool\n",
    "\n",
    "**Review**: We saw that sklearn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) can tokenize and vectorize raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorizer = vectorizer.fit_transform(docs_train)\n",
    "X_test_vectorizer = vectorizer.transform(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beijing' 'chinese' 'japan' 'macao' 'shanghai' 'tokyo']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0 0 0 0]\n",
      " [0 2 0 0 1 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectorizer.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t2\n",
      "  (0, 0)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 4)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "## vectorizer uses sparse encoding\n",
    "print(X_train_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Today**, we look at sklearn's [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), which vectorizes categorical features -- like author info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "author_v = OneHotEncoder()\n",
    "#X_train_authorv = author_v.fit_transform(authors_train)\n",
    "#OUTPUT:\n",
    "#ValueError: Expected 2D array, got 1D array instead:\n",
    "#array=['Cao' 'Wang' 'Cao' 'Hirao'].\n",
    "#Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "## note that we first need to convert this from a list to a list of list,\n",
    "## where each row stands for one example\n",
    "X_train_author_v = author_v.fit_transform([[x] for x in authors_train])\n",
    "print(X_train_author_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Cao'],\n",
       "       ['Wang'],\n",
       "       ['Cao'],\n",
       "       ['Hirao']], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## here's the original info in training\n",
    "author_v.inverse_transform(X_train_author_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "## we fit over test as well\n",
    "X_test_authors_v = author_v.transform([[x] for x in authors_test])\n",
    "print(X_test_authors_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to put everything together. Specifically, we have learnt how to create two vectorizers that hold our features, one from the actual text, one using the author information. So how do we create a single vectorizer that includes both?\n",
    "\n",
    "We use the [ColumnTransformer](https://scikit-learn.org/stable/modules/compose.html#featureunion-composite-feature-spaces). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, create a single structure for both documents and authors\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame({\"docs\": docs_train, \"authors\": authors_train})\n",
    "test_df = pd.DataFrame({\"docs\": docs_test, \"authors\": authors_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 2. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 1. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## using the ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "column_trans = ColumnTransformer(\n",
    "    [(\"words\", CountVectorizer(), \"docs\"),\n",
    "     (\"author\", OneHotEncoder(), [\"authors\"])]\n",
    ")\n",
    "\n",
    "X_train = column_trans.fit_transform(train_df)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 3. 1. 0. 0. 1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = column_trans.transform(test_df)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CountVectorizer` expects a 1D array as input and therefore the columns were specified as a string ('title'). However, `preprocessing.OneHotEncoder` as most of other transformers expects 2D data, therefore in that case you need to specify the column as a list of strings (`['authors']`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We still run Naive Bayes as usual\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding continuous features\n",
    "\n",
    "What if we want to add continuous features, say, the average length of words in each document?<br />\n",
    "How would we do it from scratch?\n",
    "\n",
    "Ok so now that we have understood the inner workings, let's use tools, which is usually much more reliable than custom implementations.\n",
    "\n",
    "First, get the average word lengths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.0, 7.333333333333333, 6.0, 5.666666666666667]\n",
      "[6.2]\n"
     ]
    }
   ],
   "source": [
    "def avg_word_len(doc):\n",
    "    return np.mean([len(w) for w in doc.split()])\n",
    "\n",
    "wl_train = [avg_word_len(doc) for doc in docs_train]\n",
    "wl_test = [avg_word_len(doc) for doc in docs_test]\n",
    "print(wl_train)\n",
    "print(wl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we update our Pandas DataFrame to put all info together again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>authors</th>\n",
       "      <th>avg_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinese Beijing Chinese</td>\n",
       "      <td>Cao</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chinese Chinese Shanghai</td>\n",
       "      <td>Wang</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chinese Macao</td>\n",
       "      <td>Cao</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tokyo Japan Chinese</td>\n",
       "      <td>Hirao</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       docs authors  avg_word_len\n",
       "0   Chinese Beijing Chinese     Cao      7.000000\n",
       "1  Chinese Chinese Shanghai    Wang      7.333333\n",
       "2             Chinese Macao     Cao      6.000000\n",
       "3       Tokyo Japan Chinese   Hirao      5.666667"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.join(pd.DataFrame({\"avg_word_len\": wl_train}))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>authors</th>\n",
       "      <th>avg_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinese Chinese Chinese Tokyo Japan</td>\n",
       "      <td>Cao</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  docs authors  avg_word_len\n",
       "0  Chinese Chinese Chinese Tokyo Japan     Cao           6.2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df.join(pd.DataFrame({\"avg_word_len\": wl_test}))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we join features with the ColumnTransformer. Note that since avg_word_len is a *continuous* feature, i.e., not a categorical feature but a real-valued one, we don't need to vectorize it. So, we tell our ColumnTransformer to ignore it. Essentially, we *only* tell ColumnTransformer which features to vectorize and how, and set `remainder='passthrough'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         2.         0.         0.         0.         0.\n",
      "  1.         0.         0.         7.        ]\n",
      " [0.         2.         0.         0.         1.         0.\n",
      "  0.         0.         1.         7.33333333]\n",
      " [0.         1.         0.         1.         0.         0.\n",
      "  1.         0.         0.         6.        ]\n",
      " [0.         1.         1.         0.         0.         1.\n",
      "  0.         1.         0.         5.66666667]]\n"
     ]
    }
   ],
   "source": [
    "column_trans = ColumnTransformer(\n",
    "    [(\"words\", CountVectorizer(), \"docs\"),\n",
    "     (\"author\", OneHotEncoder(), [\"authors\"])],\n",
    "     remainder=\"passthrough\"\n",
    ")\n",
    "X_train = column_trans.fit_transform(train_df)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  3.  1.  0.  0.  1.  1.  0.  0.  6.2]]\n"
     ]
    }
   ],
   "source": [
    "X_test = column_trans.transform(test_df)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Can MultinomialNB be used with continuous features?\n",
    "\n",
    "What should we do instead?\n",
    "\n",
    "If *all* your features are continuous, you can use the [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) classifier which puts a Gaussian prior on your features.\n",
    "\n",
    "However, our data is a *mixture* of continuous and categorical variables. We can do two things:\n",
    "\n",
    "(1) transform our continuous data into categories, aka, bin them!\n",
    "\n",
    "(2) model ensembling: build a MultinomialNB on categorical features ONLY, build a GaussianNB on the continuous features ONLY, then build another model on top.\n",
    "\n",
    "(3) Use logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
