{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning word embeddings\n",
    "\n",
    "LIN 371 :: UT Austin\n",
    "\n",
    "Original material from https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "\n",
    "This tutorial aims to:\n",
    "* How to train your own word2vec word embedding model on text data.\n",
    "* How to visualize a trained word embedding model using Principal Component Analysis.\n",
    "* How to load pre-trained word2vec and GloVe word embedding models from Google and Stanford.\n",
    "\n",
    "This notebook requires [Gensim](https://radimrehurek.com/gensim/)\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) is one algorithm for learning a word embedding from a text corpus. There are two main training algorithms that can be used to learn the embedding from text; they are continuous bag of words (CBOW) and skip grams. Word2Vec models require a lot of text, e.g. the entire Wikipedia corpus. Nevertheless, we will demonstrate the principles using a small in-memory example of text.\n",
    "\n",
    "Gensim provides [the Word2Vec class](https://radimrehurek.com/gensim/models/word2vec.html) for working with a Word2Vec model. Learning a word embedding from text involves loading and organizing the text into sentences and providing them to the constructor of a new `Word2Vec()` instance. For example:\n",
    "\n",
    "```\n",
    "sentences = ...\n",
    "model = Word2Vec(sentences)\n",
    "```\n",
    "\n",
    "Specifically, each sentence must be **tokenized**, meaning divided into words and prepared (e.g. perhaps pre-filtered and perhaps converted to a preferred case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# define training data\n",
    "text = \"When Mr. Bilbo Baggins of Bag End announced that he would shortly be celebrating his eleventy-first birthday with a party of special magnificence, there was much talk and excitement in Hobbiton. Bilbo was very rich and very peculiar, and had been the wonder of the Shire for sixty years, ever since his remarkable disappearance and unexpected return. The riches he had brought back from his travels had now become a local legend, and it was popularly believed, whatever the old folk might say, that the Hill at Bag End was full of tunnels stuffed with treasure.\"\n",
    "sentences = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "# train model\n",
    "\n",
    "# summarize the loaded model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many parameters on this constructor; a few noteworthy arguments you may wish to configure are:\n",
    "\n",
    "* **vector_size**: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
    "* **window**: (default 5) The maximum distance between a target word and words around the target word.\n",
    "* **min_count**: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
    "* **sg**: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).\n",
    "\n",
    "The defaults are often good enough when just getting started.\n",
    "\n",
    "You can print the learned vocabulary of tokens (words) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.key_to_index)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can review the embedded vector for a specific token as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model. By default, the model is saved in a binary format to save space.\n",
    "\n",
    "# load model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When getting started, you can save the learned model in ASCII format and review the contents. You can do this by setting binary=False when calling the save_word2vec_format() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model with load_word2vec_format, also specifying binary=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Word Vectors Using PCA\n",
    "\n",
    "After you learn word embedding for your text data, it can be nice to explore it with visualization. You can use PCA to reduce the high-dimensional word vectors to two-dimensional plots and plot them on a graph. The visualizations can provide a qualitative diagnostic for your learned model.\n",
    "\n",
    "We can then train a projection method on the vectors, such as those methods offered in scikit-learn, then use matplotlib to plot the projection as a scatter plot.\n",
    "\n",
    "We can retrieve all of the vectors from a trained model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a 2-dimensional PCA model of the word vectors using [the scikit-learn PCA class](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# fit a 2d PCA model to the vectors\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.key_to_index)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Google’s Word2Vec Embedding\n",
    "\n",
    "Training your own word vectors may be the best approach for a given NLP problem. But it can take a long time, a fast computer with a lot of RAM and disk space, and perhaps some expertise in finessing the input data and training algorithm. An alternative is to simply use an existing pre-trained word embedding.\n",
    "\n",
    "Along with the paper and code for word2vec, Google also published a pre-trained word2vec model on the [Word2Vec Google Code Project](https://code.google.com/archive/p/word2vec/).\n",
    "\n",
    "A pre-trained model is nothing more than a file containing tokens and their associated word vectors. The pre-trained Google word2vec model was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors. It is a 1.53 Gigabytes file. You can download it from here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\n",
    "Unzipped, the binary file (GoogleNews-vectors-negative300.bin) is 3.4 Gigabytes.\n",
    "\n",
    "The Gensim library provides tools to load this file. Specifically, you can call the `KeyedVectors.load_word2vec_format()` function to load this model into memory, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = '/GoogleNews-vectors-negative300.bin'\n",
    "filename = '/Users/jessy/Downloads/GoogleNews-vectors-negative300.bin'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that another interesting thing that you can do is do a little linear algebra arithmetic with words:\n",
    "```\n",
    "queen = (king - man) + woman\n",
    "```\n",
    "Gensim provides an interface for performing these types of operations in the `most_similar()` function on the trained or loaded model. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some other relations if they work — king:crown::police:badge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some other things - how similar are two words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What word seems out of place in the sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stanford’s GloVe Embedding\n",
    "\n",
    "Stanford researchers also have their own word embedding algorithm like word2vec called Global Vectors for Word Representation, or GloVe for short. In practice, NLP practitioners seem to prefer GloVe at the moment based on results. Like word2vec, the GloVe researchers also provide pre-trained word vectors, in this case, a great selection to choose from.\n",
    "\n",
    "You can download the GloVe pre-trained word vectors and load them easily with gensim.\n",
    "\n",
    "Let's download the smallest GloVe pre-trained model from [the GloVe website](https://nlp.stanford.edu/projects/glove/). It an 822 Megabyte zip file with 4 different models (50, 100, 200 and 300-dimensional vectors) trained on Wikipedia data with 6 billion tokens and a 400,000 word vocabulary. Direct download: http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load it and perform the same `(king – man) + woman = ?` test as in the previous section. \n",
    "\n",
    "Note that the GLOVE files are different from Word2Vec, so we set `no_header=True` when loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Stanford GloVe model\n",
    "filename = '/Users/jessy/Downloads/glove.6B/glove.6B.300d.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate: (king - man) + woman = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
